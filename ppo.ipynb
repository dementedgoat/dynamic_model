{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 1. Load Swap Events Function\n",
    "# ===========================\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import spaces\n",
    "import gym\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from arch import arch_model\n",
    "from pykalman import KalmanFilter\n",
    "import ta  # Technical Analysis library\n",
    "import tensorflow as tf\n",
    "from stable_baselines3 import PPO, DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, StopTrainingOnRewardThreshold\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', None)\n",
    "\n",
    "\n",
    "def load_events(file_path, scaling_factor=1):\n",
    "    \"\"\"\n",
    "    Loads and cleans events data from CSV, handling Burn, Mint, and Swap events.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: String, path to the swap events CSV file.\n",
    "    - scaling_factor: Float, factor to scale down large numerical values to prevent overflow.\n",
    "\n",
    "    Returns:\n",
    "    - df: DataFrame containing cleaned and feature-engineered swap events.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Events data loaded from {file_path}.\")\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"The file '{file_path}' was not found in the specified directory.\")\n",
    "\n",
    "    # Convert 'timestamp' to datetime and set as index\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "\n",
    "    # Ensure numeric fields are of the correct data type\n",
    "    numeric_fields = ['amount0', 'amount1', 'sqrtPriceX96', 'liquidity', 'tick', 'price_wstETH_ETH', 'amount']\n",
    "    for field in numeric_fields:\n",
    "        df[field] = pd.to_numeric(df[field], errors='coerce')\n",
    "\n",
    "    # Backfill anomalies in 'price_wstETH_ETH' and 'sqrtPriceX96' using 3 standard deviations\n",
    "    for column in ['price_wstETH_ETH', 'sqrtPriceX96']:\n",
    "        if column in df.columns:\n",
    "            mean = df[column].mean()\n",
    "            std = df[column].std()\n",
    "            df[column] = df[column].mask((df[column] < mean - 3 * std) | (df[column] > mean + 3 * std)).fillna(method='bfill')\n",
    "            print(f\"Backfilled anomalies in {column}.\")\n",
    "        else:\n",
    "            print(f\"Column '{column}' not found in data. Skipping backfill for this column.\")\n",
    "\n",
    "    # Compute fees (assuming amounts are in wei)\n",
    "    FEE_RATE = 0.003  # 0.3%\n",
    "    df['fee0'] = np.where(df['amount0'] > 0, df['amount0'] * FEE_RATE, 0)\n",
    "    df['fee1'] = np.where(df['amount1'] > 0, df['amount1'] * FEE_RATE, 0)\n",
    "    print(\"Fees computed.\")\n",
    "\n",
    "    # Handle event-specific fields\n",
    "    # For Swap events, 'tickLower' and 'tickUpper' may be NaN\n",
    "    # We'll create separate columns to track liquidity positions\n",
    "    df['is_mint'] = df['event'] == 'Mint'\n",
    "    df['is_burn'] = df['event'] == 'Burn'\n",
    "    df['is_swap'] = df['event'] == 'Swap'\n",
    "\n",
    "    # Calculate net liquidity changes\n",
    "    df['net_liquidity_change'] = np.where(df['is_mint'], df['amount'],\n",
    "                                          np.where(df['is_burn'], -df['amount'], 0.0))\n",
    "\n",
    "    # Set missing 'tickLower' and 'tickUpper' to 0 for Swap events\n",
    "    df['tickLower'] = df['tickLower'].fillna(0)\n",
    "    df['tickUpper'] = df['tickUpper'].fillna(0)\n",
    "\n",
    "    # Scale down large numerical values to prevent overflow\n",
    "    df['amount0'] = df['amount0'] / scaling_factor\n",
    "    df['amount1'] = df['amount1'] / scaling_factor\n",
    "    df['sqrtPriceX96'] = df['sqrtPriceX96'] / scaling_factor\n",
    "    df['liquidity'] = df['liquidity'] / scaling_factor\n",
    "    df['fee0'] = df['fee0'] / scaling_factor\n",
    "    df['fee1'] = df['fee1'] / scaling_factor\n",
    "    df['net_liquidity_change'] = df['net_liquidity_change'] / scaling_factor\n",
    "    print(f\"Scaled down numerical values by a factor of {scaling_factor}.\")\n",
    "\n",
    "    # Create additional features\n",
    "    # 1. Cumulative liquidity\n",
    "    df['cumulative_liquidity'] = df['net_liquidity_change'].cumsum()\n",
    "\n",
    "    # 2. Trading volume for Swap events\n",
    "    df['swap_volume'] = np.where(df['is_swap'], np.abs(df['amount0']) + np.abs(df['amount1']), 0.0)\n",
    "\n",
    "    # 3. Cumulative swap volume\n",
    "    df['cumulative_swap_volume'] = df['swap_volume'].cumsum()\n",
    "\n",
    "    # 4. Rolling window features (e.g., 30-day rolling volatility)\n",
    "    df['returns'] = df['price_wstETH_ETH'].pct_change().fillna(0.0)\n",
    "    df['volatility'] = df['returns'].rolling(window=30).std().fillna(0.0)\n",
    "\n",
    "    # 5. Price momentum indicators\n",
    "    df['price_ma_50'] = df['price_wstETH_ETH'].rolling(window=50).mean().fillna(method='bfill')\n",
    "    df['price_ma_200'] = df['price_wstETH_ETH'].rolling(window=200).mean().fillna(method='bfill')\n",
    "\n",
    "    # 6. Binary indicators for significant liquidity events\n",
    "    df['is_large_mint'] = (df['is_mint']) & (df['net_liquidity_change'] > df['net_liquidity_change'].quantile(0.95))\n",
    "    df['is_large_burn'] = (df['is_burn']) & (df['net_liquidity_change'] < df['net_liquidity_change'].quantile(0.05))\n",
    "\n",
    "    # 7. Fill any remaining NaNs\n",
    "    df = df.fillna(0.0)\n",
    "    print(\"Additional features engineered and missing values filled.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 2. Load Macroeconomic Data Function\n",
    "# ===========================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_macroeconomic_data(data_dir='data'):\n",
    "    \"\"\"\n",
    "    Loads and merges macroeconomic data from CSV files.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_dir: String, directory where macroeconomic CSV files are stored.\n",
    "    \n",
    "    Returns:\n",
    "    - macro_data_final: DataFrame containing merged and processed macroeconomic data.\n",
    "    \"\"\"\n",
    "    # Define file paths\n",
    "    fed_rate_path = os.path.join(data_dir, 'fed_rate.csv')\n",
    "    treasury_yield_path = os.path.join(data_dir, 'treasuries.csv')\n",
    "    sp500_path = os.path.join(data_dir, 'sp500.csv')\n",
    "    stablecoins_path = os.path.join(data_dir, 'stablecoins.csv')\n",
    "    vix_path = os.path.join(data_dir, 'vix.csv')\n",
    "    \n",
    "    # Initialize empty list to store individual DataFrames\n",
    "    dfs = []\n",
    "    \n",
    "    # Load Effective Federal Funds Rate\n",
    "    try:\n",
    "        eff_fed_rate = pd.read_csv(fed_rate_path, parse_dates=['DATE'], index_col='DATE')\n",
    "        eff_fed_rate.rename(columns={'EFFR': 'fed_rate'}, inplace=True)\n",
    "        eff_fed_rate = eff_fed_rate.apply(pd.to_numeric, errors='coerce')\n",
    "        eff_fed_rate.fillna(method='ffill', inplace=True)\n",
    "        eff_fed_rate.fillna(method='bfill', inplace=True)\n",
    "        print(\"Effective Federal Funds Rate data loaded successfully.\")\n",
    "        dfs.append(eff_fed_rate[['fed_rate']])\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"The file 'fed_rate.csv' was not found in the '{data_dir}' directory.\")\n",
    "    \n",
    "    # Load 10-Year Treasury Yields\n",
    "    try:\n",
    "        treasury_yield = pd.read_csv(treasury_yield_path, parse_dates=['DATE'], index_col='DATE')\n",
    "        treasury_yield.rename(columns={'DFII10': 'treasury_yield'}, inplace=True)\n",
    "        treasury_yield = treasury_yield.apply(pd.to_numeric, errors='coerce')\n",
    "        treasury_yield.fillna(method='ffill', inplace=True)\n",
    "        treasury_yield.fillna(method='bfill', inplace=True)\n",
    "        print(\"10-Year Treasury Yields data loaded successfully.\")\n",
    "        dfs.append(treasury_yield[['treasury_yield']])\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"The file 'treasuries.csv' was not found in the '{data_dir}' directory.\")\n",
    "    \n",
    "    # Load S&P 500 Index\n",
    "    try:\n",
    "        sp500 = pd.read_csv(sp500_path, parse_dates=['DATE'], index_col='DATE')\n",
    "        sp500.rename(columns={'SP500': 'sp500_index'}, inplace=True)\n",
    "        sp500 = sp500.apply(pd.to_numeric, errors='coerce')\n",
    "        sp500.fillna(method='ffill', inplace=True)\n",
    "        sp500.fillna(method='bfill', inplace=True)\n",
    "        print(\"S&P 500 Index data loaded successfully.\")\n",
    "        dfs.append(sp500[['sp500_index']])\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"The file 'sp500.csv' was not found in the '{data_dir}' directory.\")\n",
    "    \n",
    "    # Load Stablecoin Market Capitalization\n",
    "    try:\n",
    "        stablecoins = pd.read_csv(stablecoins_path, parse_dates=['Date'])\n",
    "        stablecoins['Timestamp'] = pd.to_datetime(stablecoins['Date'])\n",
    "        stablecoins.set_index('Timestamp', inplace=True)\n",
    "        stablecoins.rename(columns={'Total': 'stablecoin_mcap'}, inplace=True)\n",
    "        stablecoins = stablecoins.apply(pd.to_numeric, errors='coerce')\n",
    "        stablecoins.fillna(method='ffill', inplace=True)\n",
    "        stablecoins.fillna(method='bfill', inplace=True)\n",
    "        print(\"Stablecoin Market Capitalization data loaded successfully.\")\n",
    "        dfs.append(stablecoins[['stablecoin_mcap']])\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"The file 'stablecoins.csv' was not found in the '{data_dir}' directory.\")\n",
    "    \n",
    "    # Load VIX Index\n",
    "    try:\n",
    "        vix = pd.read_csv(vix_path, parse_dates=['DATE'], index_col='DATE')\n",
    "        vix.rename(columns={'VIXCLS': 'vix_index'}, inplace=True)\n",
    "        vix = vix.apply(pd.to_numeric, errors='coerce')\n",
    "        vix.fillna(method='ffill', inplace=True)\n",
    "        vix.fillna(method='bfill', inplace=True)\n",
    "        print(\"VIX Index data loaded successfully.\")\n",
    "        dfs.append(vix[['vix_index']])\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"The file 'vix.csv' was not found in the '{data_dir}' directory.\")\n",
    "    \n",
    "    # Merge all macroeconomic data into a single DataFrame\n",
    "    macro_data = pd.concat(dfs, axis=1)\n",
    "    print(\"Macroeconomic data concatenated successfully.\")\n",
    "    \n",
    "    # Resample to daily frequency and handle missing values\n",
    "    macro_data = macro_data.resample('D').mean()\n",
    "    macro_data = macro_data.apply(pd.to_numeric, errors='coerce')\n",
    "    macro_data = macro_data.fillna(method='ffill').fillna(method='bfill')\n",
    "    print(\"Macroeconomic data resampled to daily frequency and missing values filled.\")\n",
    "    \n",
    "    # Feature Engineering\n",
    "    # 1. Rate of Change for Federal Funds Rate\n",
    "    macro_data['fed_rate_change'] = macro_data['fed_rate'].pct_change().fillna(0.0)\n",
    "    macro_data['vix_change'] = macro_data['vix_index'].pct_change().fillna(0.0)\n",
    "    \n",
    "    # 2. Moving Averages\n",
    "    macro_data['treasury_yield_ma_30'] = macro_data['treasury_yield'].rolling(window=30).mean().fillna(method='bfill')\n",
    "    macro_data['sp500_index_ma_30'] = macro_data['sp500_index'].rolling(window=30).mean().fillna(method='bfill')\n",
    "    macro_data['vix_index_ma_30'] = macro_data['vix_index'].rolling(window=30).mean().fillna(method='bfill')\n",
    "    # 3. Volatility Indicators\n",
    "    macro_data['sp500_volatility'] = macro_data['sp500_index'].rolling(window=30).std().fillna(0.0)\n",
    "    \n",
    "    # 4. Interaction Features\n",
    "    macro_data['fed_treasury_yield_ratio'] = macro_data['fed_rate'] / macro_data['treasury_yield']\n",
    "    \n",
    "    # 5. Normalize Macro Features (Optional)\n",
    "    macro_features = ['fed_rate', 'treasury_yield', 'sp500_index', 'stablecoin_mcap',\n",
    "                      'fed_rate_change', 'treasury_yield_ma_30', 'sp500_index_ma_30',\n",
    "                      'sp500_volatility']\n",
    "    \n",
    "    for feature in macro_features:\n",
    "        mean = macro_data[feature].mean()\n",
    "        std = macro_data[feature].std()\n",
    "        if std != 0:\n",
    "            macro_data[feature + '_zscore'] = (macro_data[feature] - mean) / std\n",
    "        else:\n",
    "            macro_data[feature + '_zscore'] = 0.0\n",
    "        print(f\"Normalized {feature} using z-score.\")\n",
    "    \n",
    "    \n",
    "    return macro_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 3. Event Data Integration\n",
    "# ===========================\n",
    "\n",
    "def integrate_event_data(merged_df):\n",
    "    \"\"\"\n",
    "    Integrates event data into the merged DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - merged_df: DataFrame, merged swap events and macroeconomic data\n",
    "\n",
    "    Returns:\n",
    "    - merged_df: DataFrame with event indicators\n",
    "    \"\"\"\n",
    "    # Define threshold changes for event detection\n",
    "    fed_rate_threshold = 0.25  # 25 basis points\n",
    "    treasury_yield_threshold = 0.10  # 10 basis points\n",
    "    sp500_threshold = 0.02  # 2% change\n",
    "    stablecoin_mcap_threshold = 0.05  # 5% change\n",
    "\n",
    "    merged_df['total_fee'] = merged_df['fee0'] + merged_df['fee1']\n",
    "    \n",
    "    # Indicator for presence of swap\n",
    "    merged_df['has_swap'] = merged_df['is_swap'].astype(int)\n",
    "\n",
    "    # Calculate daily changes\n",
    "    merged_df['fed_rate_change'] = merged_df['fed_rate'].diff()\n",
    "    merged_df['treasury_yield_change'] = merged_df['treasury_yield'].diff()\n",
    "    merged_df['sp500_return'] = merged_df['sp500_index'].pct_change()\n",
    "    merged_df['stablecoin_mcap_change'] = merged_df['stablecoin_mcap'].pct_change()\n",
    "\n",
    "    # Create event indicators\n",
    "    merged_df['event'] = 0\n",
    "    merged_df.loc[merged_df['fed_rate_change'].abs() >= fed_rate_threshold, 'event'] = 1\n",
    "    merged_df.loc[merged_df['treasury_yield_change'].abs() >= treasury_yield_threshold, 'event'] = 1\n",
    "    merged_df.loc[merged_df['sp500_return'].abs() >= sp500_threshold, 'event'] = 1\n",
    "    merged_df.loc[merged_df['stablecoin_mcap_change'].abs() >= stablecoin_mcap_threshold, 'event'] = 1\n",
    "    print(\"\\nEvent indicators created based on defined thresholds.\")\n",
    "\n",
    "    # Infer Rebasing Events from On-Chain Data\n",
    "    merged_df['amount0_change'] = merged_df['amount0'].diff()\n",
    "\n",
    "    # Define a threshold for detecting rebasing events (e.g., 3 standard deviations)\n",
    "    rebasing_threshold = merged_df['amount0_change'].std() * 3\n",
    "    merged_df['rebasing_event'] = 0\n",
    "    merged_df.loc[merged_df['amount0_change'].abs() > rebasing_threshold, 'rebasing_event'] = 1\n",
    "    print(\"Rebasing events inferred from On-Chain data.\")\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 5. Volatility and Data Modeling\n",
    "# ===========================\n",
    "from scipy.stats import norm\n",
    "\n",
    "def calculate_volatility(merged_df):\n",
    "    \"\"\"\n",
    "    Calculates volatility using EGARCH and smooths it with Kalman Filter.\n",
    "\n",
    "    Parameters:\n",
    "    - merged_df: DataFrame, merged swap events and macroeconomic data\n",
    "\n",
    "    Returns:\n",
    "    - merged_df: DataFrame with added 'volatility' and 'kalman_volatility' columns\n",
    "    \"\"\"\n",
    "    # Calculate Log Returns\n",
    "    merged_df['returns'] = np.log(merged_df['price_wstETH_ETH'] / merged_df['price_wstETH_ETH'].shift(1))\n",
    "    merged_df.dropna(subset=['returns'], inplace=True)\n",
    "    print(\"Log returns calculated.\")\n",
    "\n",
    "    # Fit Simplified EGARCH Model\n",
    "    try:\n",
    "        egarch_model = arch_model(merged_df['returns'], vol='EGARCH', p=1, o=0, q=1, dist='normal')\n",
    "        egarch_results = egarch_model.fit(disp='off', update_freq=5)\n",
    "        print(\"Simplified EGARCH model fitted successfully.\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"EGARCH model failed to converge even after simplifying: {e}\")\n",
    "\n",
    "    # Extract conditional volatility estimates\n",
    "    merged_df['volatility'] = egarch_results.conditional_volatility\n",
    "\n",
    "    # Check for any NaNs in volatility\n",
    "    if merged_df['volatility'].isnull().any():\n",
    "        raise ValueError(\"Volatility estimates contain NaNs. Please check the EGARCH model fitting.\")\n",
    "\n",
    "    print(\"Conditional volatility estimates extracted.\")\n",
    "\n",
    "    # Assuming normal distribution of returns\n",
    "    confidence_level = 0.95\n",
    "    z_score = norm.ppf(1 - confidence_level)\n",
    "    merged_df['VaR'] = merged_df['volatility'] * z_score\n",
    "\n",
    "    # Apply Kalman Filter for Volatility Smoothing\n",
    "    observations = merged_df['volatility'].values\n",
    "\n",
    "    kf = KalmanFilter(\n",
    "        transition_matrices=[1],\n",
    "        observation_matrices=[1],\n",
    "        initial_state_mean=observations[0],\n",
    "        initial_state_covariance=1,\n",
    "        observation_covariance=1,\n",
    "        transition_covariance=0.01\n",
    "    )\n",
    "\n",
    "    state_means, state_covariances = kf.filter(observations)\n",
    "    merged_df['kalman_volatility'] = state_means.flatten()\n",
    "\n",
    "    # Verify no NaNs in kalman_volatility\n",
    "    if merged_df['kalman_volatility'].isnull().any():\n",
    "        raise ValueError(\"Kalman-filtered volatility contains NaNs.\")\n",
    "\n",
    "    print(\"Kalman filter applied to volatility estimates.\")\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dpd(merged_df, frequency='D', lp_liquidity_fraction=1.0, zscore_window=30, fee_tier_fixed=0.003):\n",
    "    \"\"\"\n",
    "    Creates the dpd DataFrame by aggregating swap events into specified frequency buckets using ETH values\n",
    "    and adding technical indicators along with z-scores for macroeconomic features.\n",
    "\n",
    "    Parameters:\n",
    "    - merged_df: DataFrame, merged swap events and macroeconomic data\n",
    "    - frequency: String, resampling frequency (e.g., 'H' for hourly, 'D' for daily)\n",
    "    - lp_liquidity_fraction: Float, fraction of total liquidity provided by the LP (0 < fraction <= 1)\n",
    "    - zscore_window: Int, window size for rolling z-score calculations\n",
    "    - fee_tier_fixed: Float, fixed fee tier for passive strategy (e.g., 0.003 for 0.3%)\n",
    "\n",
    "    Returns:\n",
    "    - dpd: DataFrame containing aggregated pool data with technical indicators and z-scores\n",
    "    \"\"\"\n",
    "    # Configure logging\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    # Ensure 'lstm_predicted_price' is filled\n",
    "    if 'lstm_predicted_price' not in merged_df.columns:\n",
    "        if 'price_wstETH_ETH' in merged_df.columns:\n",
    "            merged_df['lstm_predicted_price'] = merged_df['price_wstETH_ETH']\n",
    "            logging.info(\"'lstm_predicted_price' column not found. Filled with 'price_wstETH_ETH'.\")\n",
    "        else:\n",
    "            logging.error(\"'price_wstETH_ETH' column not found. Cannot fill 'lstm_predicted_price'.\")\n",
    "            raise KeyError(\"'price_wstETH_ETH' column not found in merged_df to fill 'lstm_predicted_price'.\")\n",
    "    merged_df['lstm_predicted_price'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "    # Check if necessary columns exist\n",
    "    required_columns = ['price_wstETH_ETH', 'volatility']\n",
    "    missing_columns = [col for col in required_columns if col not in merged_df.columns]\n",
    "    if missing_columns:\n",
    "        logging.error(f\"Column(s) {missing_columns} do not exist in merged_df\")\n",
    "        raise KeyError(f\"Column(s) {missing_columns} do not exist in merged_df\")\n",
    "\n",
    "    # List of z-score columns to include\n",
    "    zscore_columns = [\n",
    "        'fed_rate_zscore', 'treasury_yield_zscore', 'sp500_index_zscore',\n",
    "        'stablecoin_mcap_zscore', 'fed_rate_change_zscore',\n",
    "        'treasury_yield_ma_30_zscore', 'sp500_index_ma_30_zscore',\n",
    "        'sp500_volatility_zscore'\n",
    "    ]\n",
    "\n",
    "    # Verify that all z-score columns exist in merged_df\n",
    "    missing_zscore_columns = [col for col in zscore_columns if col not in merged_df.columns]\n",
    "    if missing_zscore_columns:\n",
    "        logging.error(f\"Z-Score columns {missing_zscore_columns} are missing in merged_df\")\n",
    "        raise KeyError(f\"Z-Score columns {missing_zscore_columns} are missing in merged_df\")\n",
    "\n",
    "    # Derive 'amount_in' based on event type\n",
    "    merged_df['amount_in'] = np.where(\n",
    "        merged_df['is_swap'],\n",
    "        merged_df[['amount0', 'amount1']].apply(lambda row: max(row['amount0'], 0) + max(row['amount1'], 0), axis=1),\n",
    "        0.0\n",
    "    )\n",
    "\n",
    "    # Resample to the specified frequency, including z-score columns and 'amount_in'\n",
    "    try:\n",
    "        aggregation_dict = {\n",
    "            'liquidity': 'last',\n",
    "            'price_wstETH_ETH': ['last', 'max', 'min'],\n",
    "            'fee0': 'sum',\n",
    "            'fee1': 'sum',\n",
    "            'lstm_predicted_price': 'last',\n",
    "            # 'transformer_predicted_price': 'last',\n",
    "            'volatility': 'last',\n",
    "            'kalman_volatility': 'last',\n",
    "            'tick': 'last',\n",
    "            'cumulative_liquidity': 'last',\n",
    "            'amount_in': 'sum',\n",
    "            'VaR': 'last',\n",
    "            'vix_index': 'last',\n",
    "        }\n",
    "\n",
    "        # Add z-score columns to the aggregation dictionary with 'last' as the aggregation method\n",
    "        for zcol in zscore_columns:\n",
    "            aggregation_dict[zcol] = 'last'\n",
    "\n",
    "        dpd = merged_df.resample(frequency).agg(aggregation_dict)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during resampling: {e}\")\n",
    "        raise e\n",
    "\n",
    "    # Flatten MultiIndex columns\n",
    "    dpd.columns = [\n",
    "        'liquidity', 'close', 'high', 'low',\n",
    "        'fee0_total', 'fee1_total',\n",
    "        'lstm_predicted_price', 'volatility', 'kalman_volatility',\n",
    "        'tick', 'cumulative_liquidity',\n",
    "        'amount_in', 'VaR', 'vix_index'\n",
    "    ] + zscore_columns\n",
    "\n",
    "    # Replace zero 'close' prices to avoid division by zero\n",
    "    dpd['close'].replace(0, np.nan, inplace=True)\n",
    "    dpd['close'].fillna(method='ffill', inplace=True)\n",
    "    dpd['close'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "    # Calculate ETH_price as the inverse of close price\n",
    "    with np.errstate(divide='ignore'):\n",
    "        dpd['ETH_price'] = 1 / dpd['close']\n",
    "    dpd['ETH_price'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    dpd['ETH_price'].fillna(method='ffill', inplace=True)\n",
    "    dpd['ETH_price'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "    # Handle any remaining NaNs and infinities\n",
    "    dpd.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    dpd.fillna(method='ffill', inplace=True)\n",
    "    dpd.fillna(method='bfill', inplace=True)\n",
    "\n",
    "    # Calculate the percentage change for 'liquidity'\n",
    "    dpd['liquidity_pct_change'] = dpd['liquidity'].pct_change().replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "    # Calculate LP's share of the liquidity\n",
    "    dpd['LP_liquidity_ETH'] = dpd['liquidity'] * lp_liquidity_fraction\n",
    "\n",
    "    # Calculate LP's accumulated fees\n",
    "    dpd['accumulated_fee0'] = dpd['fee0_total'] * lp_liquidity_fraction  # Already in ETH units\n",
    "    dpd['accumulated_fee1'] = dpd['fee1_total'] * lp_liquidity_fraction  # Already in ETH units\n",
    "\n",
    "    # Replace negative fees with zero (if any)\n",
    "    dpd['accumulated_fee0'] = dpd['accumulated_fee0'].clip(lower=0)\n",
    "    dpd['accumulated_fee1'] = dpd['accumulated_fee1'].clip(lower=0)\n",
    "\n",
    "    # Sum accumulated fees\n",
    "    dpd['adjusted_fee'] = dpd['accumulated_fee0'] + dpd['accumulated_fee1']\n",
    "\n",
    "    # Calculate Active Liquidity (LP's liquidity)\n",
    "    dpd['ActiveLiq'] = dpd['LP_liquidity_ETH']  # LP's liquidity in ETH\n",
    "\n",
    "    # ===============================\n",
    "    # Adding Technical Indicators\n",
    "    # ===============================\n",
    "\n",
    "    # Moving Averages\n",
    "    dpd['MA_10'] = dpd['close'].rolling(window=10, min_periods=1).mean()\n",
    "    dpd['MA_50'] = dpd['close'].rolling(window=50, min_periods=1).mean()\n",
    "    dpd['MA_200'] = dpd['close'].rolling(window=200, min_periods=1).mean()\n",
    "\n",
    "    # Relative Strength Index (RSI)\n",
    "    rsi = ta.momentum.RSIIndicator(close=dpd['close'], window=14)\n",
    "    dpd['RSI'] = rsi.rsi()\n",
    "    dpd['RSI'].fillna(method='ffill', inplace=True)\n",
    "    dpd['RSI'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "    # Bollinger Bands\n",
    "    bollinger = ta.volatility.BollingerBands(close=dpd['close'], window=20, window_dev=2)\n",
    "    dpd['BB_upper'] = bollinger.bollinger_hband()\n",
    "    dpd['BB_lower'] = bollinger.bollinger_lband()\n",
    "    dpd['BB_upper'].fillna(method='ffill', inplace=True)\n",
    "    dpd['BB_upper'].fillna(method='bfill', inplace=True)\n",
    "    dpd['BB_lower'].fillna(method='ffill', inplace=True)\n",
    "    dpd['BB_lower'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "    # ===============================\n",
    "    # Adding 'fee_tier' Column\n",
    "    # ===============================\n",
    "\n",
    "    # For Passive Strategy: Set 'fee_tier' to a fixed value (e.g., 0.003 for 0.3%)\n",
    "    dpd['fee_tier'] = fee_tier_fixed\n",
    "\n",
    "    # ===============================\n",
    "    # Final Data Integrity Checks\n",
    "    # ===============================\n",
    "\n",
    "    # Ensure all required z-score columns are present\n",
    "    final_zscore_columns = zscore_columns  # These are already z-scores\n",
    "\n",
    "    # Verify that all z-score columns are present\n",
    "    missing_final_zscore_columns = [col for col in final_zscore_columns if col not in dpd.columns]\n",
    "    if missing_final_zscore_columns:\n",
    "        logging.error(f\"Z-Score columns {missing_final_zscore_columns} are missing in dpd\")\n",
    "        raise KeyError(f\"Z-Score columns {missing_final_zscore_columns} are missing in dpd\")\n",
    "\n",
    "    # Handle any remaining NaNs after technical indicators and 'fee_tier' addition\n",
    "    dpd.fillna(method='ffill', inplace=True)\n",
    "    dpd.fillna(method='bfill', inplace=True)\n",
    "\n",
    "    # Logging the successful creation of dpd\n",
    "    logging.info(f\"dpd DataFrame created with Active Liquidity, ETH_price, volatility, liquidity_pct_change, accumulated fees, technical indicators, and fee_tier at {frequency} frequency.\")\n",
    "\n",
    "    return dpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "def build_and_train_lstm(dpd, lstm_model_path='lstm_model.keras', look_back=60):\n",
    "    \"\"\"\n",
    "    Builds and trains an improved LSTM model to predict future prices.\n",
    "\n",
    "    Parameters:\n",
    "    - dpd: DataFrame, processed dpd data\n",
    "    - lstm_model_path: String, path to save the trained LSTM model\n",
    "    - look_back: Integer, number of past timesteps to use for prediction\n",
    "\n",
    "    Returns:\n",
    "    - model: Trained LSTM model\n",
    "    - dpd: DataFrame, updated with predictions\n",
    "    \"\"\"\n",
    "    # Select features to use as input\n",
    "    features = ['close', 'volatility', 'MA_10', 'MA_50', 'RSI', 'BB_upper', 'BB_lower', 'amount_in', 'fee_tier', 'liquidity', 'VaR', 'vix_index']\n",
    "    target = 'close'\n",
    "\n",
    "    # Drop any rows with NaN values in selected features\n",
    "    dpd = dpd.dropna(subset=features)\n",
    "\n",
    "    # Convert the DataFrame to numpy array\n",
    "    dataset = dpd[features].values.astype('float32')\n",
    "\n",
    "    # Normalize the features\n",
    "    scaler = StandardScaler()\n",
    "    dataset_scaled = scaler.fit_transform(dataset)\n",
    "\n",
    "    # Split into train, validation, and test sets\n",
    "    train_size = int(len(dataset_scaled) * 0.7)\n",
    "    val_size = int(len(dataset_scaled) * 0.15)\n",
    "    test_size = len(dataset_scaled) - train_size - val_size\n",
    "\n",
    "    train, val, test = dataset_scaled[:train_size], dataset_scaled[train_size:train_size+val_size], dataset_scaled[train_size+val_size:]\n",
    "\n",
    "    # Function to create dataset\n",
    "    def create_dataset(data, look_back=1):\n",
    "        X, Y = [], []\n",
    "        for i in range(len(data)-look_back):\n",
    "            X.append(data[i:(i+look_back), :])\n",
    "            Y.append(data[i + look_back, features.index(target)])  # Index of 'close' in features\n",
    "        return np.array(X), np.array(Y)\n",
    "\n",
    "    # Create datasets\n",
    "    X_train, Y_train = create_dataset(train, look_back)\n",
    "    X_val, Y_val = create_dataset(val, look_back)\n",
    "    X_test, Y_test = create_dataset(test, look_back)\n",
    "\n",
    "    # Build LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, return_sequences=True, input_shape=(look_back, len(features))))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(64, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Train the model\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), batch_size=64, epochs=100, callbacks=[early_stop], verbose=1)\n",
    "\n",
    "    # Save the model\n",
    "    model.save(lstm_model_path)\n",
    "    print(f\"LSTM model trained and saved to {lstm_model_path}.\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    train_predict = model.predict(X_train)\n",
    "    val_predict = model.predict(X_val)\n",
    "    test_predict = model.predict(X_test)\n",
    "\n",
    "    # Invert scaling for predictions\n",
    "    def invert_scaling(X, y_pred):\n",
    "        y_pred_full = np.zeros((y_pred.shape[0], dataset.shape[1]))\n",
    "        y_pred_full[:, features.index(target)] = y_pred.flatten()\n",
    "        X_last = X[:, -1, :]\n",
    "        y_pred_full[:, [i for i in range(dataset.shape[1]) if i != features.index(target)]] = X_last[:, [i for i in range(dataset.shape[1]) if i != features.index(target)]]\n",
    "        y_pred_inv = scaler.inverse_transform(y_pred_full)[:, features.index(target)]\n",
    "        return y_pred_inv\n",
    "\n",
    "    Y_train_inv = dpd[target].values[look_back:train_size]\n",
    "    Y_val_inv = dpd[target].values[train_size+look_back:train_size+val_size]\n",
    "    Y_test_inv = dpd[target].values[train_size+val_size+look_back:]\n",
    "\n",
    "    train_predict_inv = invert_scaling(X_train, train_predict)\n",
    "    val_predict_inv = invert_scaling(X_val, val_predict)\n",
    "    test_predict_inv = invert_scaling(X_test, test_predict)\n",
    "\n",
    "    # Calculate RMSE and MAE\n",
    "    train_rmse = sqrt(mean_squared_error(Y_train_inv, train_predict_inv))\n",
    "    train_mae = mean_absolute_error(Y_train_inv, train_predict_inv)\n",
    "    val_rmse = sqrt(mean_squared_error(Y_val_inv, val_predict_inv))\n",
    "    val_mae = mean_absolute_error(Y_val_inv, val_predict_inv)\n",
    "    test_rmse = sqrt(mean_squared_error(Y_test_inv, test_predict_inv))\n",
    "    test_mae = mean_absolute_error(Y_test_inv, test_predict_inv)\n",
    "\n",
    "    print(f\"Train RMSE: {train_rmse:.4f}, MAE: {train_mae:.4f}\")\n",
    "    print(f\"Validation RMSE: {val_rmse:.4f}, MAE: {val_mae:.4f}\")\n",
    "    print(f\"Test RMSE: {test_rmse:.4f}, MAE: {test_mae:.4f}\")\n",
    "\n",
    "    # Add predictions to dpd\n",
    "    dpd['lstm_predicted_price'] = np.nan\n",
    "\n",
    "    dpd_indices = dpd.index\n",
    "\n",
    "    # For train predictions\n",
    "    train_indices = dpd_indices[look_back:train_size]\n",
    "    dpd.loc[train_indices, 'lstm_predicted_price'] = train_predict_inv\n",
    "\n",
    "    # For validation predictions\n",
    "    val_indices = dpd_indices[train_size+look_back:train_size+val_size]\n",
    "    dpd.loc[val_indices, 'lstm_predicted_price'] = val_predict_inv\n",
    "\n",
    "    # For test predictions\n",
    "    test_indices = dpd_indices[train_size+val_size+look_back:]\n",
    "    dpd.loc[test_indices, 'lstm_predicted_price'] = test_predict_inv\n",
    "\n",
    "    # Fill any remaining NaNs\n",
    "    dpd['lstm_predicted_price'].fillna(method='ffill', inplace=True)\n",
    "    dpd['lstm_predicted_price'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "    return model, dpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization, MultiHeadAttention, Dropout, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "def build_and_train_transformer(dpd, transformer_model_path='transformer_model.h5', look_back=60):\n",
    "    \"\"\"\n",
    "    Builds and trains a Transformer-based model to predict future prices.\n",
    "\n",
    "    Parameters:\n",
    "    - dpd: DataFrame, processed dpd data\n",
    "    - transformer_model_path: String, path to save the trained model\n",
    "    - look_back: Integer, number of past timesteps to use for prediction\n",
    "\n",
    "    Returns:\n",
    "    - model: Trained Transformer model\n",
    "    - dpd: DataFrame, updated with predictions\n",
    "    \"\"\"\n",
    "    from tensorflow.keras.layers import Input, Dense, LayerNormalization, MultiHeadAttention, Dropout\n",
    "    from tensorflow.keras.models import Model\n",
    "\n",
    "    # Select features to use as input\n",
    "    features = ['close', 'volatility', 'MA_10', 'MA_50', 'RSI', 'BB_upper', 'BB_lower',\n",
    "                'amount_in', 'fee_tier', 'liquidity', 'VaR', 'vix_index']\n",
    "    target = 'close'\n",
    "\n",
    "    # Drop any rows with NaN values in selected features\n",
    "    dpd = dpd.dropna(subset=features)\n",
    "\n",
    "    # Convert the DataFrame to numpy array\n",
    "    dataset = dpd[features].values.astype('float32')\n",
    "\n",
    "    # Normalize the features\n",
    "    scaler = StandardScaler()\n",
    "    dataset_scaled = scaler.fit_transform(dataset)\n",
    "\n",
    "    # Split into train, validation, and test sets\n",
    "    train_size = int(len(dataset_scaled) * 0.7)\n",
    "    val_size = int(len(dataset_scaled) * 0.15)\n",
    "    test_size = len(dataset_scaled) - train_size - val_size\n",
    "\n",
    "    train, val, test = dataset_scaled[:train_size], dataset_scaled[train_size:train_size+val_size], dataset_scaled[train_size+val_size:]\n",
    "\n",
    "    # Function to create dataset\n",
    "    def create_dataset(data, look_back=1):\n",
    "        X, Y = [], []\n",
    "        for i in range(len(data)-look_back):\n",
    "            X.append(data[i:(i+look_back), :])\n",
    "            Y.append(data[i + look_back, features.index(target)])  # Index of 'close' in features\n",
    "        return np.array(X), np.array(Y)\n",
    "\n",
    "    # Create datasets\n",
    "    X_train, Y_train = create_dataset(train, look_back)\n",
    "    X_val, Y_val = create_dataset(val, look_back)\n",
    "    X_test, Y_test = create_dataset(test, look_back)\n",
    "\n",
    "    # Build Transformer model\n",
    "    input_shape = (look_back, len(features))\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = inputs\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "    # Multi-Head Attention with compatible dimensions\n",
    "    attention_output = MultiHeadAttention(num_heads=1, key_dim=11)(x, x)  # d_model=11\n",
    "    attention_output = Dropout(0.1)(attention_output)\n",
    "    x = attention_output + x  # Skip connection\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "    # Feed Forward Network with matching dimensions\n",
    "    x_ffn = Dense(12, activation='relu')(x)\n",
    "    x_ffn = Dense(12)(x_ffn)\n",
    "    x = x_ffn + x  # Skip connection\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    outputs = Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Train the model\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), batch_size=64, epochs=100, callbacks=[early_stop], verbose=1)\n",
    "\n",
    "    # Save the model\n",
    "    model.save(transformer_model_path)\n",
    "    print(f\"Transformer model trained and saved to {transformer_model_path}.\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    train_predict = model.predict(X_train)\n",
    "    val_predict = model.predict(X_val)\n",
    "    test_predict = model.predict(X_test)\n",
    "\n",
    "    # Invert scaling for predictions\n",
    "    def invert_scaling(X, y_pred):\n",
    "        y_pred_full = np.zeros((y_pred.shape[0], dataset.shape[1]))\n",
    "        y_pred_full[:, features.index(target)] = y_pred.flatten()\n",
    "        X_last = X[:, -1, :]\n",
    "        y_pred_full[:, [i for i in range(dataset.shape[1]) if i != features.index(target)]] = X_last[:, [i for i in range(dataset.shape[1]) if i != features.index(target)]]\n",
    "        y_pred_inv = scaler.inverse_transform(y_pred_full)[:, features.index(target)]\n",
    "        return y_pred_inv\n",
    "\n",
    "    Y_train_inv = dpd[target].values[look_back:train_size]\n",
    "    Y_val_inv = dpd[target].values[train_size+look_back:train_size+val_size]\n",
    "    Y_test_inv = dpd[target].values[train_size+val_size+look_back:]\n",
    "\n",
    "    train_predict_inv = invert_scaling(X_train, train_predict)\n",
    "    val_predict_inv = invert_scaling(X_val, val_predict)\n",
    "    test_predict_inv = invert_scaling(X_test, test_predict)\n",
    "\n",
    "    # Calculate RMSE and MAE\n",
    "    train_rmse = sqrt(mean_squared_error(Y_train_inv, train_predict_inv))\n",
    "    train_mae = mean_absolute_error(Y_train_inv, train_predict_inv)\n",
    "    val_rmse = sqrt(mean_squared_error(Y_val_inv, val_predict_inv))\n",
    "    val_mae = mean_absolute_error(Y_val_inv, val_predict_inv)\n",
    "    test_rmse = sqrt(mean_squared_error(Y_test_inv, test_predict_inv))\n",
    "    test_mae = mean_absolute_error(Y_test_inv, test_predict_inv)\n",
    "\n",
    "    print(f\"Train RMSE: {train_rmse:.4f}, MAE: {train_mae:.4f}\")\n",
    "    print(f\"Validation RMSE: {val_rmse:.4f}, MAE: {val_mae:.4f}\")\n",
    "    print(f\"Test RMSE: {test_rmse:.4f}, MAE: {test_mae:.4f}\")\n",
    "\n",
    "    # Add predictions to dpd\n",
    "    dpd['lstm_predicted_price'] = np.nan\n",
    "\n",
    "    dpd_indices = dpd.index\n",
    "\n",
    "    # For train predictions\n",
    "    train_indices = dpd_indices[look_back:train_size]\n",
    "    dpd.loc[train_indices, 'lstm_predicted_price'] = train_predict_inv\n",
    "\n",
    "    # For validation predictions\n",
    "    val_indices = dpd_indices[train_size+look_back:train_size+val_size]\n",
    "    dpd.loc[val_indices, 'lstm_predicted_price'] = val_predict_inv\n",
    "\n",
    "    # For test predictions\n",
    "    test_indices = dpd_indices[train_size+val_size+look_back:]\n",
    "    dpd.loc[test_indices, 'lstm_predicted_price'] = test_predict_inv\n",
    "\n",
    "    # Fill any remaining NaNs\n",
    "    dpd['lstm_predicted_price'].fillna(method='ffill', inplace=True)\n",
    "    dpd['lstm_predicted_price'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "    return model, dpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 7. Calculate Performance Metrics Function\n",
    "# ===========================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_performance(strategies, initial_capital, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Calculates performance metrics for each strategy.\n",
    "    \n",
    "    Args:\n",
    "        strategies (dict): Dictionary containing strategies' DataFrames.\n",
    "        initial_capital (float): Initial capital for the LP.\n",
    "        start_date (pd.Timestamp): Start date of the backtest.\n",
    "        end_date (pd.Timestamp): End date of the backtest.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing performance metrics for each strategy.\n",
    "    \"\"\"\n",
    "    performance_metrics = {\n",
    "        'Total Return': [],\n",
    "        'Annualized Return': [],\n",
    "        'Volatility': [],\n",
    "        'Sharpe Ratio': [],\n",
    "        'Max Drawdown': []\n",
    "    }\n",
    "\n",
    "    for strategy_name, df in strategies.items():\n",
    "        # Ensure 'returns' column is numeric\n",
    "        df['returns'] = pd.to_numeric(df['returns'], errors='coerce').fillna(0.0)\n",
    "\n",
    "        # Calculate Total Return\n",
    "        final_value = df['total_value'].iloc[-1]\n",
    "        total_return = (final_value / initial_capital) - 1\n",
    "\n",
    "        # Calculate Annualized Return\n",
    "        days = (end_date - start_date).days\n",
    "        if days > 0:\n",
    "            annualized_return = (1 + total_return) ** (365 / days) - 1\n",
    "        else:\n",
    "            annualized_return = 0.0\n",
    "\n",
    "        # Calculate Volatility (Standard Deviation of Returns)\n",
    "        volatility = df['returns'].std() * np.sqrt(365)\n",
    "\n",
    "        # Calculate Sharpe Ratio\n",
    "        sharpe_ratio = (annualized_return) / volatility if volatility != 0 else np.nan\n",
    "\n",
    "        # Calculate Max Drawdown\n",
    "        cumulative_max = df['total_value'].cummax()\n",
    "        drawdowns = (cumulative_max - df['total_value']) / cumulative_max\n",
    "        max_drawdown = drawdowns.max()\n",
    "\n",
    "        # Append metrics\n",
    "        performance_metrics['Total Return'].append(total_return)\n",
    "        performance_metrics['Annualized Return'].append(annualized_return)\n",
    "        performance_metrics['Volatility'].append(volatility)\n",
    "        performance_metrics['Sharpe Ratio'].append(sharpe_ratio)\n",
    "        performance_metrics['Max Drawdown'].append(max_drawdown)\n",
    "\n",
    "    metrics_df = pd.DataFrame(performance_metrics, index=strategies.keys())\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 8. Visualize Results Function\n",
    "# ===========================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_results(strategies):\n",
    "    \"\"\"\n",
    "    Plots the portfolio value over time for each strategy.\n",
    "    \n",
    "    Args:\n",
    "        strategies (dict): Dictionary containing strategies' DataFrames.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for strategy_name, df in strategies.items():\n",
    "        plt.plot(df['total_value'], label=strategy_name)\n",
    "    plt.title('Portfolio Value Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Total Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# ===================================\n",
    "# Helper Functions for Passive LP\n",
    "# ===================================\n",
    "\n",
    "def tick_to_sqrt_price(tick: int) -> float:\n",
    "    try:\n",
    "        price = 1.0001 ** tick\n",
    "        sqrt_price = math.sqrt(price)\n",
    "        return sqrt_price\n",
    "    except OverflowError:\n",
    "        logging.error(f\"Overflow error during tick to sqrt price conversion for tick: {tick}\")\n",
    "        return 0.0\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in tick_to_sqrt_price: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def get_amounts_passive(sqrt_current: float, sqrtA: float, sqrtB: float, liquidity: float) -> tuple:\n",
    "    try:\n",
    "        if sqrt_current <= sqrtA:\n",
    "            amount0 = liquidity * (sqrtB - sqrtA) / (sqrtB * sqrtA)\n",
    "            amount1 = 0.0\n",
    "        elif sqrtA < sqrt_current < sqrtB:\n",
    "            amount0 = liquidity * (sqrtB - sqrt_current) / (sqrtB * sqrt_current)\n",
    "            amount1 = liquidity * (sqrt_current - sqrtA)\n",
    "        else:\n",
    "            amount0 = 0.0\n",
    "            amount1 = liquidity * (sqrtB - sqrtA)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in get_amounts_passive: {e}\")\n",
    "        amount0, amount1 = 0.0, 0.0\n",
    "    return amount0, amount1\n",
    "\n",
    "def calculate_liquidity_passive(sqrt_current: float, sqrtA: float, sqrtB: float, amount0: float, amount1: float) -> float:\n",
    "    try:\n",
    "        if sqrt_current <= sqrtA:\n",
    "            liquidity = amount0 * (sqrtB * sqrtA) / (sqrtB - sqrtA)\n",
    "        elif sqrtA < sqrt_current < sqrtB:\n",
    "            liquidity0 = amount0 * (sqrt_current * sqrtB) / (sqrtB - sqrt_current)\n",
    "            liquidity1 = amount1 / (sqrt_current - sqrtA)\n",
    "            liquidity = min(liquidity0, liquidity1)\n",
    "        else:\n",
    "            liquidity = amount1 * (sqrtB * sqrtA) / (sqrtB - sqrtA)\n",
    "    except ZeroDivisionError:\n",
    "        logging.error(f\"ZeroDivisionError in calculate_liquidity_passive with amount0={amount0}, amount1={amount1}\")\n",
    "        liquidity = 0.0\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in calculate_liquidity_passive: {e}\")\n",
    "        liquidity = 0.0\n",
    "    return liquidity\n",
    "\n",
    "def generate_price_ranges(current_tick: int, tick_spacing: int, multipliers: list = [2, 5, 10]) -> list:\n",
    "    price_ranges = []\n",
    "    for multiplier in multipliers:\n",
    "        range_width = tick_spacing * multiplier\n",
    "        tick_lower = current_tick - range_width\n",
    "        tick_upper = current_tick + range_width\n",
    "        tick_lower = (tick_lower // tick_spacing) * tick_spacing\n",
    "        tick_upper = (tick_upper // tick_spacing) * tick_spacing\n",
    "        price_ranges.append((tick_lower, tick_upper))\n",
    "    return price_ranges\n",
    "\n",
    "def scale_dpd(dpd: pd.DataFrame) -> pd.DataFrame:\n",
    "    dpd_scaled = dpd.copy()\n",
    "    columns_to_scale = [\n",
    "        'liquidity', 'cumulative_liquidity', 'amount_in',\n",
    "        'accumulated_fee0', 'accumulated_fee1', 'LP_liquidity_ETH',\n",
    "        'fee0_total', 'fee1_total', 'adjusted_fee'\n",
    "    ]\n",
    "    \n",
    "    for col in columns_to_scale:\n",
    "        if col in dpd_scaled.columns:\n",
    "            dpd_scaled[col] = dpd_scaled[col] / 1e18\n",
    "            logging.debug(f\"Scaled column '{col}' to ETH.\")\n",
    "        else:\n",
    "            logging.warning(f\"Column '{col}' not found in DPD DataFrame. Skipping scaling for this column.\")\n",
    "    \n",
    "    return dpd_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_passive_lp(\n",
    "    dpd: pd.DataFrame,\n",
    "    initial_capital: float = 10.0,         # Adjusted for user example\n",
    "    rebalance_frequency: int = 2000,       # Rebalancing every 2000 days (~5.48 years)\n",
    "    fee_tier: float = 0.003,                # Fixed fee tier (e.g., 0.3%)\n",
    "    allocation_fraction: float = 1.0,       # Allocation fraction set to 100%\n",
    "    tick_spacing: int = 100,                # Tick spacing set to 100\n",
    ") -> pd.DataFrame:\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    if not logger.handlers:\n",
    "        handler = logging.StreamHandler()\n",
    "        handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "        logger.addHandler(handler)\n",
    "    \n",
    "    # Initialize tracking DataFrame\n",
    "    passive_df = pd.DataFrame(index=dpd.index, columns=['total_value', 'returns', 'fees_earned'])\n",
    "    passive_df.iloc[0]['total_value'] = initial_capital\n",
    "    passive_df.iloc[0]['returns'] = 0.0\n",
    "    passive_df.iloc[0]['fees_earned'] = 0.0\n",
    "    \n",
    "    portfolio_value = initial_capital\n",
    "    accumulated_fees = 0.0\n",
    "    done = False\n",
    "    \n",
    "    positions = []\n",
    "    position_id_counter = 0\n",
    "    \n",
    "    # Initialize with a default position in the middle price range\n",
    "    current_tick = dpd.iloc[0]['tick']\n",
    "    price_ranges = generate_price_ranges(current_tick, tick_spacing)\n",
    "    selected_range_index = len(price_ranges) // 2\n",
    "    tick_lower, tick_upper = price_ranges[selected_range_index]\n",
    "    \n",
    "    # Allocate funds to open the initial position\n",
    "    investment_value = portfolio_value * allocation_fraction\n",
    "    amount0 = investment_value * 0.5  # 50% in token0\n",
    "    close_price = dpd.iloc[0]['close']\n",
    "    if close_price == 0:\n",
    "        logger.error(\"Close price is zero. Cannot calculate amount1 for initial position.\")\n",
    "        amount1 = 0.0\n",
    "    else:\n",
    "        amount1 = (investment_value * 0.5) / close_price\n",
    "    \n",
    "    sqrt_current = tick_to_sqrt_price(current_tick)\n",
    "    sqrtA = tick_to_sqrt_price(tick_lower)\n",
    "    sqrtB = tick_to_sqrt_price(tick_upper)\n",
    "    \n",
    "    liquidity = calculate_liquidity_passive(sqrt_current, sqrtA, sqrtB, amount0, amount1)\n",
    "    if liquidity > 0:\n",
    "        portfolio_value -= investment_value  # Deduct invested amount\n",
    "        positions.append({\n",
    "            'id': position_id_counter,\n",
    "            'tick_lower': tick_lower,\n",
    "            'tick_upper': tick_upper,\n",
    "            'liquidity': liquidity\n",
    "        })\n",
    "        logger.debug(f\"Opened Initial Position {position_id_counter}: Tick Range ({tick_lower}, {tick_upper}), Liquidity: {liquidity} ETH\")\n",
    "        position_id_counter += 1\n",
    "    else:\n",
    "        logger.error(\"Initial liquidity calculation failed. No position opened.\")\n",
    "    \n",
    "    # Simulation Loop\n",
    "    for step in range(1, len(dpd)):\n",
    "        if done:\n",
    "            passive_df.iloc[step]['total_value'] = 0.0\n",
    "            passive_df.iloc[step]['returns'] = -1.0\n",
    "            passive_df.iloc[step]['fees_earned'] = 0.0\n",
    "            continue\n",
    "        \n",
    "        current_tick = dpd.iloc[step]['tick']\n",
    "        price = dpd.iloc[step]['close']\n",
    "        swap_amount_in = dpd.iloc[step]['amount_in']\n",
    "        active_liquidity_pool = dpd.iloc[step]['liquidity']\n",
    "        \n",
    "        # Rebalance if frequency is met\n",
    "        if step % rebalance_frequency == 0:\n",
    "            logger.info(f\"Passive LP - Step {step}: Rebalancing Positions\")\n",
    "            # Close all positions\n",
    "            total_amount0, total_amount1 = 0.0, 0.0\n",
    "            for pos in positions:\n",
    "                sqrtA_pos = tick_to_sqrt_price(pos['tick_lower'])\n",
    "                sqrtB_pos = tick_to_sqrt_price(pos['tick_upper'])\n",
    "                amount0_pos, amount1_pos = get_amounts_passive(\n",
    "                    sqrt_current=sqrt_current,\n",
    "                    sqrtA=sqrtA_pos,\n",
    "                    sqrtB=sqrtB_pos,\n",
    "                    liquidity=pos['liquidity']\n",
    "                )\n",
    "                total_amount0 += amount0_pos\n",
    "                total_amount1 += amount1_pos\n",
    "                logger.debug(f\"Closed Position {pos['id']}: Returned Amount0={amount0_pos} ETH, Amount1={amount1_pos} ETH\")\n",
    "            \n",
    "            # Update portfolio value\n",
    "            if price == 0:\n",
    "                # logger.error(f\"Step {step}: Close price is zero. Cannot calculate portfolio value from amount1.\")\n",
    "                portfolio_value += total_amount0\n",
    "            else:\n",
    "                value_from_amount0 = total_amount0\n",
    "                value_from_amount1 = total_amount1 * price\n",
    "                total_value = value_from_amount0 + value_from_amount1\n",
    "                portfolio_value += total_value\n",
    "                # logger.debug(f\"Step {step}: Portfolio Value after Closing Positions: {portfolio_value} ETH\")\n",
    "            \n",
    "            # Clear positions\n",
    "            positions = []\n",
    "            \n",
    "            # Generate new price ranges based on current_tick\n",
    "            price_ranges = generate_price_ranges(current_tick, tick_spacing)\n",
    "            selected_range_index = len(price_ranges) // 2\n",
    "            tick_lower, tick_upper = price_ranges[selected_range_index]\n",
    "            \n",
    "            # Open new position\n",
    "            investment_value = portfolio_value * allocation_fraction\n",
    "            amount0 = investment_value * 0.5\n",
    "            if price == 0:\n",
    "                logger.error(f\"Step {step}: Close price is zero. Cannot calculate amount1 for new position.\")\n",
    "                amount1 = 0.0\n",
    "            else:\n",
    "                amount1 = (investment_value * 0.5) / price\n",
    "            \n",
    "            sqrt_current = tick_to_sqrt_price(current_tick)\n",
    "            sqrtA = tick_to_sqrt_price(tick_lower)\n",
    "            sqrtB = tick_to_sqrt_price(tick_upper)\n",
    "            \n",
    "            liquidity = calculate_liquidity_passive(sqrt_current, sqrtA, sqrtB, amount0, amount1)\n",
    "            if liquidity > 0:\n",
    "                portfolio_value -= investment_value\n",
    "                positions.append({\n",
    "                    'id': position_id_counter,\n",
    "                    'tick_lower': tick_lower,\n",
    "                    'tick_upper': tick_upper,\n",
    "                    'liquidity': liquidity\n",
    "                })\n",
    "                logger.debug(f\"Opened New Position {position_id_counter}: Tick Range ({tick_lower}, {tick_upper}), Liquidity: {liquidity} ETH\")\n",
    "                position_id_counter += 1\n",
    "            else:\n",
    "                logger.error(f\"Step {step}: Liquidity calculation failed. No new position opened.\")\n",
    "        \n",
    "        # Calculate fees earned\n",
    "        if active_liquidity_pool <= 0:\n",
    "            fee_share = 0.0\n",
    "            # logger.error(f\"Step {step}: Invalid active liquidity pool: {active_liquidity_pool}. Setting fee_share to 0.\")\n",
    "        else:\n",
    "            total_liquidity_lp = sum([pos['liquidity'] for pos in positions])\n",
    "            fee_share = min(total_liquidity_lp / active_liquidity_pool, 1.0)\n",
    "        \n",
    "        fee_accrued = swap_amount_in * fee_tier * fee_share\n",
    "        accumulated_fees += fee_accrued\n",
    "        portfolio_value += fee_accrued\n",
    "        \n",
    "        # Prevent portfolio value from exceeding a maximum threshold\n",
    "        max_portfolio_threshold = 1e12  # Example threshold; adjust as needed\n",
    "        if portfolio_value > max_portfolio_threshold:\n",
    "            # logger.warning(f\"Step {step}: Portfolio value exceeded threshold: {portfolio_value}. Capping to {max_portfolio_threshold}.\")\n",
    "            portfolio_value = max_portfolio_threshold\n",
    "        \n",
    "        # Record portfolio value and fees earned\n",
    "        passive_df.iloc[step]['total_value'] = portfolio_value\n",
    "        passive_df.iloc[step]['fees_earned'] = fee_accrued\n",
    "        \n",
    "        # Calculate returns\n",
    "        prev_total = passive_df.iloc[step - 1]['total_value']\n",
    "        if prev_total != 0:\n",
    "            passive_df.iloc[step]['returns'] = (portfolio_value - prev_total) / prev_total\n",
    "        else:\n",
    "            passive_df.iloc[step]['returns'] = 0.0\n",
    "        \n",
    "        # Check if portfolio value falls below minimum threshold\n",
    "        if portfolio_value < 0.0:\n",
    "            # logger.error(f\"Step {step}: Portfolio value negative. Terminating simulation.\")\n",
    "            passive_df.iloc[step]['total_value'] = 0.0\n",
    "            passive_df.iloc[step]['returns'] = -1.0\n",
    "            passive_df.iloc[step]['fees_earned'] = 0.0\n",
    "            done = True\n",
    "        \n",
    "        # Log only significant steps to reduce verbosity\n",
    "        if step % 1000 == 0 or step == len(dpd) - 1:\n",
    "            logger.info(f\"Passive LP - Step {step}: Portfolio Value: {portfolio_value:.6f} ETH | Fees Earned: {fee_accrued:.6f} ETH\")\n",
    "    \n",
    "    # After the simulation loop, withdraw all remaining liquidity\n",
    "    if positions:\n",
    "        logger.info(\"Withdrawing all remaining liquidity at the end of simulation.\")\n",
    "        total_amount0, total_amount1 = 0.0, 0.0\n",
    "        for pos in positions:\n",
    "            sqrtA_pos = tick_to_sqrt_price(pos['tick_lower'])\n",
    "            sqrtB_pos = tick_to_sqrt_price(pos['tick_upper'])\n",
    "            amount0_pos, amount1_pos = get_amounts_passive(\n",
    "                sqrt_current=sqrt_current,\n",
    "                sqrtA=sqrtA_pos,\n",
    "                sqrtB=sqrtB_pos,\n",
    "                liquidity=pos['liquidity']\n",
    "            )\n",
    "            total_amount0 += amount0_pos\n",
    "            total_amount1 += amount1_pos\n",
    "            logger.debug(f\"Withdrew Position {pos['id']}: Returned Amount0={amount0_pos} ETH, Amount1={amount1_pos} ETH\")\n",
    "        \n",
    "        # Calculate the total value from withdrawn liquidity\n",
    "        if close_price == 0:\n",
    "            logger.error(\"Final close price is zero. Cannot calculate portfolio value from withdrawn amount1.\")\n",
    "            portfolio_value += total_amount0\n",
    "        else:\n",
    "            value_from_amount0 = total_amount0\n",
    "            value_from_amount1 = total_amount1 * close_price\n",
    "            total_withdrawn_value = value_from_amount0 + value_from_amount1\n",
    "            portfolio_value += total_withdrawn_value\n",
    "            logger.debug(f\"Final Withdrawal: Added {total_withdrawn_value} ETH to portfolio.\")\n",
    "        \n",
    "        # Update the last row in passive_df with the final portfolio value\n",
    "        passive_df.iloc[-1]['total_value'] = portfolio_value\n",
    "        passive_df.iloc[-1]['fees_earned'] += 0.0  # No additional fees from withdrawal\n",
    "    \n",
    "    logger.info(\"Passive LP simulation completed with final liquidity withdrawal.\")\n",
    "    \n",
    "    return passive_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import logging\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import deque\n",
    "\n",
    "class LPRebalanceEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Environment for Uniswap V3 Liquidity Provider Rebalancing.\n",
    "    Incorporates LSTM predicted price and volatility along with macro and technical variables\n",
    "    to dynamically adjust swap fees and proactively rebalance LP positions.\n",
    "    Includes a feedback loop to adjust rebalancing thresholds based on performance metrics and VIX index.\n",
    "    \"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dpd: pd.DataFrame,\n",
    "        initial_capital: float = 10.0,\n",
    "        transaction_cost: float = 0.003,\n",
    "        gas_cost_mint: float = 0.001,\n",
    "        fee_tiers: list = [0.003, 0.005, 0.01, 0.015],\n",
    "        allocation_fraction: float = 0.5,  # Set to 0.5 as per user observation\n",
    "        min_portfolio_value: float = 1.0,\n",
    "        rebalance_threshold: float = 0.015,  # Threshold for actual price change to trigger rebalancing\n",
    "        vaR_confidence_level: float = 0.95,  # Confidence level for VaR\n",
    "        rebalance_cooldown: int = 1,        # Minimum steps between rebalances\n",
    "        volatility_multiplier: float = 1.0   # Multiplier for volatility when setting price ranges\n",
    "    ):\n",
    "        super(LPRebalanceEnv, self).__init__()\n",
    "\n",
    "        # Configure logging\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "        # Store parameters\n",
    "        self.dpd = dpd.reset_index(drop=True)\n",
    "        self.initial_capital = initial_capital\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.gas_cost_mint = gas_cost_mint\n",
    "        self.fee_tiers = fee_tiers\n",
    "        self.allocation_fraction = allocation_fraction\n",
    "        self.min_portfolio_value = min_portfolio_value\n",
    "        self.rebalance_threshold = rebalance_threshold \n",
    "        self.vaR_confidence_level = vaR_confidence_level\n",
    "        self.rebalance_cooldown = rebalance_cooldown\n",
    "        self.volatility_multiplier = volatility_multiplier\n",
    "\n",
    "        self.total_steps = len(self.dpd)\n",
    "        self.current_step = 0\n",
    "        self.current_price = self.dpd.iloc[self.current_step]['close']\n",
    "\n",
    "        # Initialize total initial amounts\n",
    "        self.total_initial_amount0 = 0.0\n",
    "        self.total_initial_amount1 = 0.0\n",
    "\n",
    "        # Initialize positions\n",
    "        self.positions = []\n",
    "        self.position_id_counter = 0\n",
    "\n",
    "        # Initialize portfolio\n",
    "        self.portfolio_value = self.initial_capital\n",
    "        self.accumulated_fees = 0.0\n",
    "        self.gas_costs = 0.0\n",
    "\n",
    "        # Initialize fee tier\n",
    "        self.current_fee_tier = self.fee_tiers[0]\n",
    "        self.fee_update_count = 0  # Counter for fee tier updates\n",
    "\n",
    "        # For cooldown between rebalances\n",
    "        self.last_rebalance_step = 0\n",
    "        self.last_rebalance_price = self.current_price\n",
    "\n",
    "        # Define observation features including LSTM predicted price, volatility, VaR, and VIX index\n",
    "        self.obs_features = [\n",
    "            'close', 'volatility', 'MA_10', 'MA_50', 'RSI', 'BB_upper', 'BB_lower',\n",
    "            'fed_rate_zscore', 'treasury_yield_zscore', 'sp500_index_zscore',\n",
    "            'stablecoin_mcap_zscore', 'fed_rate_change_zscore',\n",
    "            'treasury_yield_ma_30_zscore', 'sp500_index_ma_30_zscore',\n",
    "            'sp500_volatility_zscore', 'amount_in', 'fee_tier', 'liquidity',\n",
    "            'lstm_predicted_price', 'VaR', 'vix_index'\n",
    "        ]\n",
    "\n",
    "        # Define action space (placeholder since we're using a rule-based approach)\n",
    "        self.action_space = spaces.Discrete(1)\n",
    "\n",
    "        # Fit scaler\n",
    "        self.scaler = StandardScaler()\n",
    "        try:\n",
    "            self.scaler.fit(self.dpd[self.obs_features])\n",
    "        except ValueError as e:\n",
    "            logging.error(f\"Error fitting scaler: {e}\")\n",
    "            self.scaler = None\n",
    "\n",
    "        # Define the observation space\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(len(self.obs_features),), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.done = False\n",
    "\n",
    "        # ===========================\n",
    "        # Feedback Loop Initialization\n",
    "        # ===========================\n",
    "        self.performance_window = 100  # Number of steps to consider for performance metrics\n",
    "        self.portfolio_history = deque(maxlen=self.performance_window)\n",
    "        self.returns_history = deque(maxlen=self.performance_window)\n",
    "        self.sharpe_ratio = 0.0\n",
    "        self.sortino_ratio = 0.0\n",
    "        self.max_drawdown = 0.0\n",
    "        # ===========================\n",
    "\n",
    "        # Reset environment\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the environment to the initial state.\n",
    "        \"\"\"\n",
    "        self.current_step = 0\n",
    "        self.current_price = self.dpd.iloc[self.current_step]['close']\n",
    "        self.portfolio_value = self.initial_capital\n",
    "        self.accumulated_fees = 0.0\n",
    "        self.gas_costs = 0.0\n",
    "        self.done = False\n",
    "\n",
    "        # Clear positions\n",
    "        self.positions = []\n",
    "        self.position_id_counter = 0\n",
    "\n",
    "        # Reset total initial amounts\n",
    "        self.total_initial_amount0 = 0.0\n",
    "        self.total_initial_amount1 = 0.0\n",
    "\n",
    "        # Reset fee tier updates\n",
    "        self.fee_update_count = 0\n",
    "\n",
    "        # Reset last rebalance info\n",
    "        self.last_rebalance_step = 0\n",
    "        self.last_rebalance_price = self.current_price\n",
    "\n",
    "        # ===========================\n",
    "        # Reset Feedback Loop Data\n",
    "        # ===========================\n",
    "        self.portfolio_history.clear()\n",
    "        self.returns_history.clear()\n",
    "        self.sharpe_ratio = 0.0\n",
    "        self.sortino_ratio = 0.0\n",
    "        self.max_drawdown = 0.0\n",
    "        # ===========================\n",
    "\n",
    "        # Open initial position\n",
    "        self.open_new_position()\n",
    "\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Executes one time step within the environment.\n",
    "        Rebalances positions based on actual price movements and whether the current price is outside the active tick range.\n",
    "        Tracks the number of fee tier updates and adjusts rebalance_threshold based on performance metrics and VIX index.\n",
    "        \"\"\"\n",
    "        if self.done:\n",
    "            return self._get_obs(), 0.0, True, {}\n",
    "\n",
    "        # Advance to next step\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= self.total_steps:\n",
    "            self.done = True\n",
    "            self.withdraw_all_positions()\n",
    "            return self._get_obs(), 0.0, True, {}\n",
    "\n",
    "        # Update current price\n",
    "        self.current_price = self.dpd.iloc[self.current_step]['close']\n",
    "\n",
    "        # Get LSTM predicted price and volatility\n",
    "        lstm_predicted_price = self.dpd.iloc[self.current_step]['lstm_predicted_price']\n",
    "        egarch_volatility = self.dpd.iloc[self.current_step]['volatility']\n",
    "        var = self.dpd.iloc[self.current_step]['VaR']\n",
    "        vix = self.dpd.iloc[self.current_step]['vix_index']  # Using VIX index\n",
    "\n",
    "        # Adjust fee tier based on predicted volatility\n",
    "        previous_fee_tier = self.current_fee_tier\n",
    "        if egarch_volatility < 0.015:\n",
    "            self.current_fee_tier = 0.003\n",
    "        elif 0.015 <= egarch_volatility < 0.020:\n",
    "            self.current_fee_tier = 0.005\n",
    "        elif 0.020 <= egarch_volatility < 0.025:\n",
    "            self.current_fee_tier = 0.01\n",
    "        else:\n",
    "            self.current_fee_tier = 0.015\n",
    "\n",
    "        # Adjust allocation_fraction based on predicted volatility\n",
    "        if egarch_volatility < 0.015:\n",
    "            self.allocation_fraction = 0.95\n",
    "        elif 0.015 <= egarch_volatility < 0.020:\n",
    "            self.allocation_fraction = 0.85\n",
    "        elif 0.020 <= egarch_volatility < 0.025:\n",
    "            self.allocation_fraction = 0.8\n",
    "        else:\n",
    "            self.allocation_fraction = 0.7\n",
    "\n",
    "        # Track fee tier updates\n",
    "        if self.current_fee_tier != previous_fee_tier:\n",
    "            self.fee_update_count += 1\n",
    "            logging.info(f\"Step {self.current_step}: Fee tier adjusted from {previous_fee_tier} to {self.current_fee_tier}\")\n",
    "\n",
    "        # Calculate fees earned\n",
    "        fees_earned = self.calculate_fees()\n",
    "        fees_earned = max(fees_earned, 0.0) if np.isfinite(fees_earned) else 0.0\n",
    "\n",
    "        # Update portfolio value\n",
    "        self.portfolio_value += fees_earned\n",
    "        self.accumulated_fees += fees_earned\n",
    "\n",
    "        # Check if cooldown period has passed\n",
    "        cooldown_passed = (self.current_step - self.last_rebalance_step) >= self.rebalance_cooldown\n",
    "\n",
    "        # Determine if current price is outside the active tick range\n",
    "        outside_tick_range = False\n",
    "        for pos in self.positions:\n",
    "            if not (pos['price_lower'] <= self.current_price <= pos['price_upper']):\n",
    "                outside_tick_range = True\n",
    "                break\n",
    "\n",
    "        # Calculate actual price change since last rebalance\n",
    "        actual_price_change = (self.current_price - self.last_rebalance_price) / self.last_rebalance_price\n",
    "\n",
    "        rebalanced = False\n",
    "\n",
    "        if cooldown_passed and outside_tick_range and abs(actual_price_change) >= self.rebalance_threshold:\n",
    "            logging.info(f\"Step {self.current_step}: Rebalance condition met with actual price change of {actual_price_change:.2%}\")\n",
    "\n",
    "            # Simulate potential IL before rebalancing\n",
    "            current_il = self.calculate_impermanent_loss()\n",
    "            potential_il = self.simulate_potential_impermanent_loss()\n",
    "\n",
    "            if potential_il <= current_il:\n",
    "                # Proceed with rebalancing\n",
    "                self.close_all_positions()\n",
    "                self.open_new_position()\n",
    "                rebalanced = True\n",
    "                self.last_rebalance_step = self.current_step\n",
    "                self.last_rebalance_price = self.current_price\n",
    "                logging.info(f\"Step {self.current_step}: Rebalanced positions.\")\n",
    "            else:\n",
    "                logging.info(f\"Step {self.current_step}: Potential IL higher than current IL. Skipping rebalance.\")\n",
    "\n",
    "        # Calculate Impermanent Loss (IL) for monitoring\n",
    "        impermanent_loss = self.calculate_impermanent_loss()\n",
    "\n",
    "        # Calculate Portfolio Growth Rate\n",
    "        portfolio_growth_rate = (self.portfolio_value - self.initial_capital) / self.initial_capital\n",
    "\n",
    "        # ===========================\n",
    "        # Feedback Loop: Update Performance Metrics and Adjust Rebalance Threshold\n",
    "        # ===========================\n",
    "        self.portfolio_history.append(portfolio_growth_rate)\n",
    "        self.returns_history.append(portfolio_growth_rate)\n",
    "\n",
    "        if len(self.returns_history) == self.performance_window:\n",
    "            returns = np.array(self.returns_history)\n",
    "            mean_return = np.mean(returns)\n",
    "            std_return = np.std(returns)\n",
    "            downside_returns = returns[returns < 0]\n",
    "            std_downside = np.std(downside_returns) if len(downside_returns) > 0 else 1e-6\n",
    "\n",
    "            self.sharpe_ratio = mean_return / std_return if std_return != 0 else 0.0\n",
    "            self.sortino_ratio = mean_return / std_downside if std_downside != 0 else 0.0\n",
    "\n",
    "            peak = np.maximum.accumulate(np.array(self.portfolio_history))\n",
    "            drawdowns = (peak - np.array(self.portfolio_history)) / peak\n",
    "            self.max_drawdown = np.max(drawdowns) if len(drawdowns) > 0 else 0.0\n",
    "\n",
    "            # Adjust rebalance_threshold based on Sharpe Ratio\n",
    "            if self.sharpe_ratio < 0.5 and self.rebalance_threshold < 0.05:\n",
    "                self.rebalance_threshold += 0.005\n",
    "                logging.info(f\"Adjusted rebalance_threshold to {self.rebalance_threshold:.3f} based on Sharpe Ratio.\")\n",
    "            elif self.sharpe_ratio > 1.0 and self.rebalance_threshold > 0.01:\n",
    "                self.rebalance_threshold -= 0.005\n",
    "                logging.info(f\"Adjusted rebalance_threshold to {self.rebalance_threshold:.3f} based on Sharpe Ratio.\")\n",
    "\n",
    "            # Adjust rebalance_threshold based on VIX Index\n",
    "            if vix > 25 and self.rebalance_threshold < 0.05:\n",
    "                self.rebalance_threshold += 0.002\n",
    "                logging.info(f\"Adjusted rebalance_threshold to {self.rebalance_threshold:.3f} based on high VIX index.\")\n",
    "            elif vix < 20 and self.rebalance_threshold > 0.01:\n",
    "                self.rebalance_threshold -= 0.002\n",
    "                logging.info(f\"Adjusted rebalance_threshold to {self.rebalance_threshold:.3f} based on low VIX index.\")\n",
    "        # ===========================\n",
    "\n",
    "        # Compute Reward\n",
    "        # Penalize positive IL and high VaR\n",
    "        reward = portfolio_growth_rate + (fees_earned / self.initial_capital) \\\n",
    "                 - (max(impermanent_loss, 0) * 0.5) - (var * 0.5)\n",
    "\n",
    "        # Ensure portfolio value is valid\n",
    "        if self.portfolio_value <= 0:\n",
    "            self.portfolio_value = 0.0\n",
    "            self.done = True\n",
    "            reward -= 1000.0  # Large negative reward to penalize\n",
    "\n",
    "        # Get next observation\n",
    "        observation = self._get_obs()\n",
    "\n",
    "        # Info dictionary\n",
    "        info = {\n",
    "            'portfolio_value': self.portfolio_value,\n",
    "            'fees_earned': fees_earned,\n",
    "            'impermanent_loss': impermanent_loss,\n",
    "            'fee_tier': self.current_fee_tier,\n",
    "            'VaR': var,\n",
    "            'Rebalanced': rebalanced,\n",
    "            'fee_updates': self.fee_update_count,\n",
    "            'sharpe_ratio': self.sharpe_ratio,\n",
    "            'sortino_ratio': self.sortino_ratio,\n",
    "            'max_drawdown': self.max_drawdown\n",
    "        }\n",
    "\n",
    "        # Check if done\n",
    "        if self.current_step >= self.total_steps - 1:\n",
    "            self.done = True\n",
    "            self.withdraw_all_positions()\n",
    "\n",
    "        return observation, reward, self.done, info\n",
    "\n",
    "    def open_new_position(self):\n",
    "        \"\"\"\n",
    "        Opens a new liquidity position based on current price, EGARCH volatility, and VIX index.\n",
    "        Adjusts the volatility multiplier based on VIX to set adaptive price ranges.\n",
    "        \"\"\"\n",
    "        current_price = self.current_price\n",
    "        egarch_volatility = self.dpd.iloc[self.current_step]['volatility']\n",
    "        vix = self.dpd.iloc[self.current_step]['vix_index']  # Using VIX index\n",
    "\n",
    "        # Adjust volatility multiplier based on VIX\n",
    "        if vix > 25:\n",
    "            adjusted_multiplier = self.volatility_multiplier * 1.5\n",
    "            logging.info(f\"High VIX detected ({vix}). Increasing volatility multiplier to {adjusted_multiplier:.2f}.\")\n",
    "        elif vix < 14:\n",
    "            adjusted_multiplier = self.volatility_multiplier * 0.8  \n",
    "            logging.info(f\"Low VIX detected ({vix}). Decreasing volatility multiplier to {adjusted_multiplier:.2f}.\")\n",
    "        else:\n",
    "            adjusted_multiplier = self.volatility_multiplier  \n",
    "            logging.info(f\"Moderate VIX detected ({vix}). Keeping volatility multiplier at {adjusted_multiplier:.2f}.\")\n",
    "\n",
    "        # Determine price range based on adjusted volatility multiplier\n",
    "        price_lower = current_price * (1 - egarch_volatility * adjusted_multiplier)\n",
    "        price_upper = current_price * (1 + egarch_volatility * adjusted_multiplier)\n",
    "\n",
    "        # Ensure price_lower is positive\n",
    "        if price_lower <= 0:\n",
    "            price_lower = current_price * 0.95  # Set a floor at 95% of current price\n",
    "            logging.warning(f\"Calculated price_lower is non-positive. Adjusted to {price_lower:.2f}.\")\n",
    "\n",
    "        # Allocate investment\n",
    "        investment_value = self.portfolio_value * self.allocation_fraction\n",
    "        amount0 = investment_value * 0.5\n",
    "        if current_price == 0:\n",
    "            logging.error(\"Current price is zero. Cannot calculate amount1 for new position.\")\n",
    "            amount1 = 0.0\n",
    "        else:\n",
    "            amount1 = (investment_value * 0.5) / current_price\n",
    "\n",
    "        # Calculate liquidity\n",
    "        sqrtA = math.sqrt(price_lower)\n",
    "        sqrtB = math.sqrt(price_upper)\n",
    "        sqrt_current = math.sqrt(current_price)\n",
    "\n",
    "        liquidity = self.calculate_liquidity(sqrt_current, sqrtA, sqrtB, amount0, amount1)\n",
    "\n",
    "        # Validate liquidity\n",
    "        if liquidity <= 0 or not np.isfinite(liquidity):\n",
    "            logging.error(f\"Invalid liquidity calculated: {liquidity}. Skipping position.\")\n",
    "            return\n",
    "\n",
    "        # Update portfolio value\n",
    "        total_cost = investment_value + self.gas_cost_mint\n",
    "        if total_cost > self.portfolio_value:\n",
    "            logging.warning(f\"Not enough portfolio value to open new position. Required: {total_cost}, Available: {self.portfolio_value}\")\n",
    "            return\n",
    "        self.portfolio_value -= total_cost\n",
    "        self.gas_costs += self.gas_cost_mint\n",
    "\n",
    "        # Update total initial amounts\n",
    "        self.total_initial_amount0 += amount0\n",
    "        self.total_initial_amount1 += amount1\n",
    "\n",
    "        # Add position\n",
    "        position = {\n",
    "            'id': self.position_id_counter,\n",
    "            'price_lower': price_lower,\n",
    "            'price_upper': price_upper,\n",
    "            'liquidity': liquidity,\n",
    "            'fee_tier': self.current_fee_tier,\n",
    "            'initial_amount0': amount0,\n",
    "            'initial_amount1': amount1,\n",
    "            'initial_price': self.current_price  # Store initial price for IL calculation\n",
    "        }\n",
    "        self.positions.append(position)\n",
    "        self.position_id_counter += 1\n",
    "\n",
    "        logging.info(f\"Opened new position {position['id']} with price range [{price_lower:.2f}, {price_upper:.2f}] and fee tier {self.current_fee_tier}\")\n",
    "\n",
    "    def close_all_positions(self):\n",
    "        \"\"\"\n",
    "        Closes all active liquidity positions and updates the portfolio value.\n",
    "        \"\"\"\n",
    "        if not self.positions:\n",
    "            return\n",
    "\n",
    "        total_amount0 = 0.0\n",
    "        total_amount1 = 0.0\n",
    "        for pos in self.positions:\n",
    "            sqrtA = math.sqrt(pos['price_lower']) if pos['price_lower'] > 0 else math.sqrt(self.current_price * 0.95)\n",
    "            sqrtB = math.sqrt(pos['price_upper'])\n",
    "            sqrt_current = math.sqrt(self.current_price)\n",
    "\n",
    "            amount0, amount1 = self.get_amounts(sqrt_current, sqrtA, sqrtB, pos['liquidity'])\n",
    "\n",
    "            # Validate amounts\n",
    "            if not (np.isfinite(amount0) and np.isfinite(amount1)):\n",
    "                logging.error(f\"Invalid amounts calculated for position {pos['id']}: amount0={amount0}, amount1={amount1}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            total_amount0 += amount0\n",
    "            total_amount1 += amount1\n",
    "\n",
    "            # Update total initial amounts\n",
    "            self.total_initial_amount0 -= pos['initial_amount0']\n",
    "            self.total_initial_amount1 -= pos['initial_amount1']\n",
    "\n",
    "            logging.info(f\"Closed position {pos['id']}: amount0={amount0:.6f}, amount1={amount1:.6f}\")\n",
    "\n",
    "        # Convert amounts to portfolio value\n",
    "        value_from_amount0 = total_amount0\n",
    "        value_from_amount1 = total_amount1 * self.current_price\n",
    "\n",
    "        total_withdrawn_value = value_from_amount0 + value_from_amount1\n",
    "        self.portfolio_value += total_withdrawn_value\n",
    "\n",
    "        # Clear positions\n",
    "        self.positions = []\n",
    "\n",
    "        logging.info(f\"Closed all positions. Withdrawn value: {total_withdrawn_value:.6f} ETH\")\n",
    "\n",
    "    def calculate_fees(self):\n",
    "        \"\"\"\n",
    "        Calculates the fees earned by the LP based on their liquidity share.\n",
    "        \"\"\"\n",
    "        current_data = self.dpd.iloc[self.current_step]\n",
    "        swap_amount_in = current_data['amount_in']\n",
    "        swap_fee_tier = current_data['fee_tier']\n",
    "        active_liquidity_pool = current_data['liquidity']\n",
    "\n",
    "        # Validate active liquidity pool\n",
    "        if active_liquidity_pool <= 0 or not np.isfinite(active_liquidity_pool):\n",
    "            fee_share = 0.0\n",
    "        else:\n",
    "            total_liquidity_lp = sum([pos['liquidity'] for pos in self.positions])\n",
    "            fee_share = min(total_liquidity_lp / active_liquidity_pool, 1.0)\n",
    "\n",
    "        # Calculate fee accrued\n",
    "        fee_accrued = swap_amount_in * swap_fee_tier * fee_share\n",
    "\n",
    "        # Validate fee accrued\n",
    "        if fee_accrued < 0 or not np.isfinite(fee_accrued):\n",
    "            fee_accrued = 0.0\n",
    "\n",
    "        return fee_accrued\n",
    "\n",
    "    def calculate_value_hold_initial_liquidity(self) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the value of holding the initial amounts of tokens at current prices.\n",
    "        \"\"\"\n",
    "        value_hold = self.total_initial_amount0 + self.total_initial_amount1 * self.current_price\n",
    "        return value_hold\n",
    "\n",
    "    def calculate_value_lp_tokens(self) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the current value of the LP tokens at current prices.\n",
    "        \"\"\"\n",
    "        total_amount0 = 0.0\n",
    "        total_amount1 = 0.0\n",
    "\n",
    "        for pos in self.positions:\n",
    "            sqrtA = math.sqrt(pos['price_lower']) if pos['price_lower'] > 0 else math.sqrt(self.current_price * 0.95)\n",
    "            sqrtB = math.sqrt(pos['price_upper'])\n",
    "            sqrt_current = math.sqrt(self.current_price)\n",
    "\n",
    "            amount0, amount1 = self.get_amounts(sqrt_current, sqrtA, sqrtB, pos['liquidity'])\n",
    "\n",
    "            total_amount0 += amount0\n",
    "            total_amount1 += amount1\n",
    "\n",
    "        value_lp = total_amount0 + total_amount1 * self.current_price\n",
    "        return value_lp\n",
    "\n",
    "    def calculate_impermanent_loss(self) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the impermanent loss as a percentage for Uniswap V3.\n",
    "        Considers each position's current holdings based on the current price.\n",
    "        \"\"\"\n",
    "        if not self.positions:\n",
    "            return 0.0\n",
    "\n",
    "        total_value_hold = 0.0\n",
    "        total_value_lp = 0.0\n",
    "\n",
    "        for pos in self.positions:\n",
    "            initial_price = pos.get('initial_price', self.current_price)\n",
    "            amount0_initial = pos['initial_amount0']\n",
    "            amount1_initial = pos['initial_amount1']\n",
    "\n",
    "            # Value if holding assets\n",
    "            value_hold = amount0_initial + amount1_initial * initial_price\n",
    "            total_value_hold += value_hold\n",
    "\n",
    "            # Current holdings based on current price\n",
    "            current_price = self.current_price\n",
    "            sqrtA = math.sqrt(pos['price_lower']) if pos['price_lower'] > 0 else math.sqrt(current_price * 0.95)\n",
    "            sqrtB = math.sqrt(pos['price_upper'])\n",
    "            sqrt_current = math.sqrt(current_price)\n",
    "\n",
    "            amount0_current, amount1_current = self.get_amounts(sqrt_current, sqrtA, sqrtB, pos['liquidity'])\n",
    "\n",
    "            value_lp = amount0_current + amount1_current * current_price\n",
    "            total_value_lp += value_lp\n",
    "\n",
    "        # Calculate overall IL\n",
    "        if total_value_hold > 0:\n",
    "            il_percentage = ((total_value_lp / total_value_hold) - 1) * 100\n",
    "        else:\n",
    "            il_percentage = 0.0\n",
    "\n",
    "        return il_percentage\n",
    "\n",
    "    def simulate_potential_impermanent_loss(self) -> float:\n",
    "        \"\"\"\n",
    "        Simulates the potential impermanent loss if a new position is opened.\n",
    "        \"\"\"\n",
    "        # For simplicity, assume new position has same allocation and price range as would be set in open_new_position\n",
    "        current_price = self.current_price\n",
    "        egarch_volatility = self.dpd.iloc[self.current_step]['volatility']\n",
    "        vix = self.dpd.iloc[self.current_step]['vix_index']  # Using VIX index\n",
    "\n",
    "        # Adjust volatility multiplier based on VIX\n",
    "        if vix > 25:\n",
    "            adjusted_multiplier = self.volatility_multiplier * 1.2  # Increase multiplier during high volatility\n",
    "        elif vix < 20:\n",
    "            adjusted_multiplier = self.volatility_multiplier * 0.8  # Decrease multiplier during low volatility\n",
    "        else:\n",
    "            adjusted_multiplier = self.volatility_multiplier  # No change\n",
    "\n",
    "        # Determine price range based on adjusted volatility multiplier\n",
    "        price_lower = current_price * (1 - egarch_volatility * adjusted_multiplier)\n",
    "        price_upper = current_price * (1 + egarch_volatility * adjusted_multiplier)\n",
    "\n",
    "        # Ensure price_lower is positive\n",
    "        if price_lower <= 0:\n",
    "            price_lower = current_price * 0.95  # Set a floor at 95% of current price\n",
    "\n",
    "        # Allocate investment\n",
    "        investment_value = self.portfolio_value * self.allocation_fraction\n",
    "        amount0 = investment_value * 0.5\n",
    "        if current_price == 0:\n",
    "            logging.error(\"Current price is zero. Cannot calculate amount1 for potential IL simulation.\")\n",
    "            amount1 = 0.0\n",
    "        else:\n",
    "            amount1 = (investment_value * 0.5) / current_price\n",
    "\n",
    "        # Simulate price movement\n",
    "        simulated_price = current_price * (1 + egarch_volatility)\n",
    "\n",
    "        # Recalculate amounts at new price\n",
    "        sqrtA = math.sqrt(price_lower)\n",
    "        sqrtB = math.sqrt(price_upper)\n",
    "        sqrt_simulated = math.sqrt(simulated_price)\n",
    "\n",
    "        liquidity = self.calculate_liquidity(sqrt_simulated, sqrtA, sqrtB, amount0, amount1)\n",
    "        if liquidity <= 0 or not np.isfinite(liquidity):\n",
    "            logging.error(f\"Invalid liquidity calculated during potential IL simulation: {liquidity}.\")\n",
    "            return float('inf')  # Return a high IL to prevent rebalancing\n",
    "\n",
    "        amount0_new, amount1_new = self.get_amounts(sqrt_simulated, sqrtA, sqrtB, liquidity)\n",
    "\n",
    "        value_lp_new = amount0_new + amount1_new * simulated_price\n",
    "        value_hold_new = amount0 + amount1 * simulated_price\n",
    "\n",
    "        if value_hold_new > 0:\n",
    "            il_percentage = ((value_lp_new / value_hold_new) - 1) * 100\n",
    "        else:\n",
    "            il_percentage = 0.0\n",
    "\n",
    "        return il_percentage\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        Retrieves the current observation.\n",
    "        \"\"\"\n",
    "        if self.current_step >= self.total_steps:\n",
    "            return np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "\n",
    "        row = self.dpd.iloc[self.current_step]\n",
    "        features = [row[feature] for feature in self.obs_features]\n",
    "        obs = np.array(features, dtype=np.float32)\n",
    "        obs = np.nan_to_num(obs, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        if self.scaler:\n",
    "            try:\n",
    "                obs = self.scaler.transform([obs])[0]\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error during scaling observation: {e}. Setting to zero vector.\")\n",
    "                obs = np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        return obs\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"\n",
    "        Renders the environment.\n",
    "        \"\"\"\n",
    "        print(f\"Step: {self.current_step}\")\n",
    "        print(f\"Portfolio Value: {self.portfolio_value:.6f} ETH\")\n",
    "        print(f\"Accumulated Fees: {self.accumulated_fees:.6f} ETH\")\n",
    "        print(f\"Gas Costs: {self.gas_costs:.6f} ETH\")\n",
    "        print(f\"Positions: {self.positions}\")\n",
    "        print(f\"Fee Tier Updates: {self.fee_update_count}\")\n",
    "        print(f\"Sharpe Ratio: {self.sharpe_ratio:.4f}\")\n",
    "        print(f\"Sortino Ratio: {self.sortino_ratio:.4f}\")\n",
    "        print(f\"Max Drawdown: {self.max_drawdown:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Performs any necessary cleanup.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    # Helper Functions\n",
    "\n",
    "    def get_amount0(self, sqrtA: float, sqrtB: float, liquidity: float) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the amount of token0 for a given liquidity and price range.\n",
    "        \"\"\"\n",
    "        if sqrtA > sqrtB:\n",
    "            sqrtA, sqrtB = sqrtB, sqrtA\n",
    "        try:\n",
    "            amount0 = liquidity * (sqrtB - sqrtA) / (sqrtB * sqrtA)\n",
    "            return amount0\n",
    "        except ZeroDivisionError:\n",
    "            logging.error(\"Division by zero in get_amount0.\")\n",
    "            return 0.0\n",
    "\n",
    "    def get_amount1(self, sqrtA: float, sqrtB: float, liquidity: float) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the amount of token1 for a given liquidity and price range.\n",
    "        \"\"\"\n",
    "        if sqrtA > sqrtB:\n",
    "            sqrtA, sqrtB = sqrtB, sqrtA\n",
    "        amount1 = liquidity * (sqrtB - sqrtA)\n",
    "        return amount1\n",
    "\n",
    "    def get_amounts(self, sqrt_current: float, sqrtA: float, sqrtB: float, liquidity: float) -> tuple:\n",
    "        \"\"\"\n",
    "        Determines the amounts of token0 and token1 based on current price and liquidity.\n",
    "        \"\"\"\n",
    "        if sqrt_current <= sqrtA:\n",
    "            amount0 = self.get_amount0(sqrtA, sqrtB, liquidity)\n",
    "            amount1 = 0.0\n",
    "        elif sqrt_current < sqrtB:\n",
    "            amount0 = self.get_amount0(sqrt_current, sqrtB, liquidity)\n",
    "            amount1 = self.get_amount1(sqrtA, sqrt_current, liquidity)\n",
    "        else:\n",
    "            amount0 = 0.0\n",
    "            amount1 = self.get_amount1(sqrtA, sqrtB, liquidity)\n",
    "        return amount0, amount1\n",
    "\n",
    "    def calculate_liquidity(self, sqrt_current: float, sqrtA: float, sqrtB: float, amount0: float, amount1: float) -> float:\n",
    "        \"\"\"\n",
    "        Calculates liquidity based on provided amounts and price ranges.\n",
    "        \"\"\"\n",
    "        if sqrt_current <= sqrtA:\n",
    "            liquidity = amount0 * sqrtA * sqrtB / (sqrtB - sqrtA)\n",
    "        elif sqrt_current < sqrtB:\n",
    "            liquidity0 = amount0 * sqrt_current * sqrtB / (sqrtB - sqrt_current)\n",
    "            liquidity1 = amount1 / (sqrt_current - sqrtA)\n",
    "            liquidity = min(liquidity0, liquidity1)\n",
    "        else:\n",
    "            liquidity = amount1 / (sqrtB - sqrtA)\n",
    "        return liquidity\n",
    "\n",
    "    def withdraw_all_positions(self):\n",
    "        \"\"\"\n",
    "        Withdraws all positions at the end of the simulation.\n",
    "        \"\"\"\n",
    "        self.close_all_positions()\n",
    "        logging.info(f\"Withdrew all positions: Portfolio Value is now {self.portfolio_value:.6f} ETH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def implement_strategies(\n",
    "    dpd: pd.DataFrame,\n",
    "    initial_capital: float = 10.0,  # Starting with 10 ETH for demonstration\n",
    "    model=None  # Trained PPO model\n",
    "):\n",
    "    \"\"\"\n",
    "    Implements Dynamic LP and Passive LP strategies using the LPRebalanceEnv environment.\n",
    "    \n",
    "    - Dynamic LP uses the trained RL model to adjust liquidity ranges based on predicted price and volatility.\n",
    "    - Passive LP maintains a fixed liquidity range throughout the simulation.\n",
    "    \n",
    "    Additionally tracks starting and ending portfolio values, total fees earned, \n",
    "    the number of fee tier updates, the number of rebalances, and impermanent loss for Dynamic LP.\n",
    "    \n",
    "    Args:\n",
    "        dpd (pd.DataFrame): Preprocessed swap events data (already scaled and includes LSTM predictions).\n",
    "        initial_capital (float): Initial capital for the LP.\n",
    "        model: Trained PPO model for Dynamic LP strategy.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing performance metrics for each strategy.\n",
    "        pd.DataFrame: The processed data DataFrame.\n",
    "    \"\"\"\n",
    "    # Configure logging to show only essential information\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    if not logger.handlers:\n",
    "        handler = logging.StreamHandler()\n",
    "        handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "        logger.addHandler(handler)\n",
    "    \n",
    "    # ===========================\n",
    "    # Initialize Strategies DataFrames and Metrics\n",
    "    # ===========================\n",
    "    strategies = {\n",
    "        'Dynamic LP': pd.DataFrame(index=dpd.index, columns=['total_value', 'returns', 'fees_earned', 'impermanent_loss']),\n",
    "        'Passive LP': pd.DataFrame(index=dpd.index, columns=['total_value', 'returns', 'fees_earned']),\n",
    "    }\n",
    "\n",
    "    # Set initial capital and returns\n",
    "    for strategy in strategies:\n",
    "        strategies[strategy].iloc[0]['total_value'] = initial_capital\n",
    "        strategies[strategy].iloc[0]['returns'] = 0.0\n",
    "        strategies[strategy].iloc[0]['fees_earned'] = 0.0\n",
    "        if strategy == 'Dynamic LP':\n",
    "            strategies[strategy].iloc[0]['impermanent_loss'] = 0.0\n",
    "\n",
    "    # Initialize tracking for Dynamic LP\n",
    "    dynamic_rebalance_count = 0\n",
    "    dynamic_fee_updates = 0\n",
    "    dynamic_impermanent_loss = 0.0  # To track final IL\n",
    "\n",
    "    # ===========================\n",
    "    # Simulate Dynamic LP Strategy\n",
    "    # ===========================\n",
    "    if model is not None:\n",
    "        # Initialize Environment\n",
    "        env_dynamic = LPRebalanceEnv(\n",
    "            dpd=dpd,\n",
    "            initial_capital=initial_capital,\n",
    "            transaction_cost=0.003,      # 0.3% Uniswap fee\n",
    "            allocation_fraction=0.9,    # 90% allocation\n",
    "        )\n",
    "        logging.info(\"Dynamic LP environment initialized with trained model.\")\n",
    "\n",
    "        # Reset Environment\n",
    "        obs_dynamic = env_dynamic.reset()\n",
    "        done_dynamic = False\n",
    "        step_dynamic = 0\n",
    "\n",
    "        while not done_dynamic:\n",
    "            # Use the trained model to predict the action\n",
    "            try:\n",
    "                action, _states = model.predict(obs_dynamic, deterministic=True)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error during model prediction: {e}\")\n",
    "                action = None  # Default to no action if prediction fails\n",
    "\n",
    "            # Execute step in the environment\n",
    "            try:\n",
    "                obs_dynamic, reward_dynamic, done_dynamic, info_dynamic = env_dynamic.step(action)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error during environment step for Dynamic LP: {e}\")\n",
    "                raise e\n",
    "\n",
    "            # Record portfolio value, fees earned, and impermanent loss\n",
    "            current_step_dynamic = min(step_dynamic + 1, len(dpd) - 1)\n",
    "            portfolio_value_dynamic = info_dynamic.get('portfolio_value', initial_capital)\n",
    "            fees_earned_dynamic = info_dynamic.get('fees_earned', 0.0)\n",
    "            impermanent_loss_dynamic = info_dynamic.get('impermanent_loss', 0.0)\n",
    "            fee_updates_dynamic = info_dynamic.get('fee_updates', 0)\n",
    "\n",
    "            # Ensure variables are floats\n",
    "            portfolio_value_dynamic = float(portfolio_value_dynamic)\n",
    "            fees_earned_dynamic = float(fees_earned_dynamic)\n",
    "            impermanent_loss_dynamic = float(impermanent_loss_dynamic)\n",
    "            fee_updates_dynamic = int(fee_updates_dynamic)\n",
    "\n",
    "            # Update DataFrame\n",
    "            strategies['Dynamic LP'].iloc[current_step_dynamic]['total_value'] = portfolio_value_dynamic\n",
    "            strategies['Dynamic LP'].iloc[current_step_dynamic]['fees_earned'] = fees_earned_dynamic\n",
    "            strategies['Dynamic LP'].iloc[current_step_dynamic]['impermanent_loss'] = impermanent_loss_dynamic\n",
    "\n",
    "            # Calculate returns\n",
    "            prev_total_dynamic = strategies['Dynamic LP'].iloc[current_step_dynamic - 1]['total_value']\n",
    "            if prev_total_dynamic != 0:\n",
    "                strategies['Dynamic LP'].iloc[current_step_dynamic]['returns'] = (\n",
    "                    portfolio_value_dynamic - prev_total_dynamic\n",
    "                ) / prev_total_dynamic\n",
    "            else:\n",
    "                strategies['Dynamic LP'].iloc[current_step_dynamic]['returns'] = 0.0\n",
    "\n",
    "            # Track rebalances\n",
    "            if info_dynamic.get('Rebalanced', False):\n",
    "                dynamic_rebalance_count += 1\n",
    "\n",
    "            # Track fee tier updates\n",
    "            dynamic_fee_updates = fee_updates_dynamic\n",
    "\n",
    "            # Track impermanent loss\n",
    "            dynamic_impermanent_loss = impermanent_loss_dynamic\n",
    "\n",
    "            # Log only significant steps to reduce verbosity\n",
    "            if done_dynamic or step_dynamic == 0 or step_dynamic % 1000 == 0:\n",
    "                logging.info(f\"Dynamic LP - Step {step_dynamic}: Portfolio Value: {portfolio_value_dynamic:.6f} ETH | Fees Earned: {fees_earned_dynamic:.6f} ETH | IL: {impermanent_loss_dynamic:.2f}% | Fee Updates: {fee_updates_dynamic}\")\n",
    "\n",
    "            step_dynamic += 1\n",
    "            if step_dynamic >= len(dpd) - 1:\n",
    "                done_dynamic = True\n",
    "\n",
    "        # After the loop, capture the final portfolio value post-withdrawal\n",
    "        final_portfolio_value_dynamic = env_dynamic.portfolio_value\n",
    "        strategies['Dynamic LP'].iloc[-1]['total_value'] = final_portfolio_value_dynamic\n",
    "        strategies['Dynamic LP'].iloc[-1]['fees_earned'] = env_dynamic.accumulated_fees\n",
    "        dynamic_impermanent_loss = final_portfolio_value_dynamic - env_dynamic.accumulated_fees\n",
    "        logging.info(f\"Final Dynamic LP Portfolio Value after withdrawal: {final_portfolio_value_dynamic:.6f} ETH\")\n",
    "        logging.info(f\"Total Rebalances: {dynamic_rebalance_count}\")\n",
    "        logging.info(f\"Total Fee Tier Updates: {dynamic_fee_updates}\")\n",
    "        logging.info(f\"Final Impermanent Loss: {dynamic_impermanent_loss:.2f}%\")\n",
    "    else:\n",
    "        logging.warning(\"No model provided for Dynamic LP. Skipping Dynamic LP strategy.\")\n",
    "\n",
    "    # ===========================\n",
    "    # Simulate Passive LP Strategy\n",
    "    # ===========================\n",
    "    try:\n",
    "        passive_df = simulate_passive_lp(\n",
    "            dpd=dpd,\n",
    "            initial_capital=initial_capital,\n",
    "            rebalance_frequency=2000,    # Rebalance every 2000 steps; adjust as needed\n",
    "            fee_tier=0.003,               # Fixed fee tier (e.g., 0.3%)\n",
    "            allocation_fraction=0.9,      # 80% allocation\n",
    "            tick_spacing=150\n",
    "        )\n",
    "\n",
    "        # Merge Passive LP results into strategies DataFrame\n",
    "        strategies['Passive LP']['total_value'] = passive_df['total_value']\n",
    "        strategies['Passive LP']['fees_earned'] = passive_df['fees_earned']\n",
    "        strategies['Passive LP']['returns'] = passive_df['returns']\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during Passive LP simulation: {e}\")\n",
    "        raise e\n",
    "\n",
    "    # ===========================\n",
    "    # Calculate Performance Metrics\n",
    "    # ===========================\n",
    "    performance_metrics = {}\n",
    "    for strategy, df in strategies.items():\n",
    "        # Starting and Ending Portfolio Values\n",
    "        starting_value = df['total_value'].iloc[0]\n",
    "        ending_value = df['total_value'].iloc[-1]\n",
    "\n",
    "        # Total Fees Earned\n",
    "        total_fees = df['fees_earned'].sum()\n",
    "\n",
    "        # Total Return\n",
    "        total_return = (ending_value - starting_value) / starting_value if starting_value != 0 else np.nan\n",
    "\n",
    "        # Annualized Return\n",
    "        annualized_return = (1 + total_return) ** (365 / len(dpd)) - 1 if len(dpd) > 0 else np.nan\n",
    "\n",
    "        # Volatility\n",
    "        volatility = df['returns'].std() * np.sqrt(365) if df['returns'].std() else np.nan\n",
    "\n",
    "        # Sharpe Ratio\n",
    "        sharpe_ratio = annualized_return / volatility if volatility and volatility != 0 else np.nan\n",
    "\n",
    "        # Max Drawdown\n",
    "        cumulative_max = df['total_value'].cummax()\n",
    "        drawdown = (cumulative_max - df['total_value']) / cumulative_max\n",
    "        max_drawdown = drawdown.max() if not drawdown.isnull().all() else np.nan\n",
    "\n",
    "        performance_metrics[strategy] = {\n",
    "            'Starting Portfolio Value': starting_value,\n",
    "            'Ending Portfolio Value': ending_value,\n",
    "            'Total Fees Earned': total_fees,\n",
    "            'Total Return': total_return,\n",
    "            'Annualized Return': annualized_return,\n",
    "            'Volatility': volatility,\n",
    "            'Sharpe Ratio': sharpe_ratio,\n",
    "            'Max Drawdown': max_drawdown,\n",
    "            'Impermanent Loss': starting_value + total_fees - ending_value,\n",
    "        }\n",
    "\n",
    "    # Add rebalances count and fee updates for Dynamic LP\n",
    "    if 'Dynamic LP' in performance_metrics and model is not None:\n",
    "        performance_metrics['Dynamic LP']['Rebalances Count'] = dynamic_rebalance_count\n",
    "        performance_metrics['Dynamic LP']['Fee Tier Updates'] = dynamic_fee_updates\n",
    "\n",
    "    if 'Passive LP' in performance_metrics and model is not None:\n",
    "        performance_metrics['Passive LP']['Rebalances Count'] = 0\n",
    "        performance_metrics['Passive LP']['Fee Tier Updates'] = 0\n",
    "\n",
    "    # Convert performance metrics to DataFrame for better visualization\n",
    "    metrics_df = pd.DataFrame(performance_metrics).T\n",
    "\n",
    "    # Log performance metrics\n",
    "    logger.info(\"Performance Metrics:\")\n",
    "    logger.info(metrics_df)\n",
    "\n",
    "    # ===========================\n",
    "    # Visualize Results\n",
    "    # ===========================\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for strategy, df in strategies.items():\n",
    "        plt.plot(df['total_value'], label=strategy)\n",
    "\n",
    "    plt.title('Portfolio Value Over Time')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Portfolio Value (ETH)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    logger.info(\"Portfolio Value Over Time plotted.\")\n",
    "\n",
    "    # Plot Impermanent Loss Over Time for Dynamic LP\n",
    "    if 'Dynamic LP' in strategies and model is not None:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(strategies['Dynamic LP']['impermanent_loss'], label='Impermanent Loss (%)', color='orange')\n",
    "        plt.xlabel('Time Steps')\n",
    "        plt.ylabel('Impermanent Loss (%)')\n",
    "        plt.title('Impermanent Loss Over Time - Dynamic LP')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        logger.info(\"Impermanent Loss Over Time plotted for Dynamic LP.\")\n",
    "\n",
    "    return metrics_df, dpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "\n",
    "def optimize_ppo(trial, dpd):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna to optimize PPO hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.trial.Trial): Optuna trial object.\n",
    "        dpd (pd.DataFrame): Preprocessed data for the environment.\n",
    "\n",
    "    Returns:\n",
    "        float: Mean reward achieved by the PPO model.\n",
    "    \"\"\"\n",
    "    import gym\n",
    "    from stable_baselines3 import PPO\n",
    "    from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "    from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "    # Suggest hyperparameters\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n",
    "    gamma = trial.suggest_float('gamma', 0.90, 0.999)\n",
    "    gae_lambda = trial.suggest_float('gae_lambda', 0.80, 0.99)\n",
    "    ent_coef = trial.suggest_loguniform('ent_coef', 1e-8, 1e-2)\n",
    "    n_steps = trial.suggest_categorical('n_steps', [512, 1024, 2048])\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    n_epochs = trial.suggest_int('n_epochs', 3, 15)\n",
    "    vf_coef = trial.suggest_float('vf_coef', 0.1, 1.0)\n",
    "    max_grad_norm = trial.suggest_float('max_grad_norm', 0.1, 0.9)\n",
    "\n",
    "    # Create the environment\n",
    "    env = DummyVecEnv([lambda: LPRebalanceEnv(dpd=dpd)])\n",
    "\n",
    "    # Initialize the PPO model with suggested hyperparameters\n",
    "    model = PPO(\n",
    "        'MlpPolicy',\n",
    "        env,\n",
    "        learning_rate=learning_rate,\n",
    "        gamma=gamma,\n",
    "        gae_lambda=gae_lambda,\n",
    "        ent_coef=ent_coef,\n",
    "        n_steps=n_steps,\n",
    "        batch_size=batch_size,\n",
    "        n_epochs=n_epochs,\n",
    "        vf_coef=vf_coef,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    try:\n",
    "        model.learn(total_timesteps=10000, reset_num_timesteps=True)\n",
    "    except Exception as e:\n",
    "        trial.report(float('-inf'), step=0)\n",
    "        return float('-inf')\n",
    "\n",
    "    # Evaluate the model\n",
    "    try:\n",
    "        mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=10, deterministic=True)\n",
    "    except Exception as e:\n",
    "        mean_reward = float('-inf')\n",
    "\n",
    "    # Close the environment\n",
    "    env.close()\n",
    "\n",
    "    return mean_reward\n",
    "\n",
    "def run_optuna_optimization_ppo(dpd, n_trials=20):\n",
    "    \"\"\"\n",
    "    Runs Optuna optimization to find the best PPO hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        dpd (pd.DataFrame): Preprocessed data for the environment.\n",
    "        n_trials (int): Number of Optuna trials.\n",
    "\n",
    "    Returns:\n",
    "        optuna.study.Study: The completed Optuna study.\n",
    "    \"\"\"\n",
    "    from optuna.pruners import MedianPruner\n",
    "\n",
    "    def objective(trial):\n",
    "        return optimize_ppo(trial, dpd)\n",
    "\n",
    "    # Set up a pruner to stop unpromising trials early\n",
    "    pruner = MedianPruner(n_startup_trials=8, n_warmup_steps=10)\n",
    "\n",
    "    # Create the Optuna study with the pruner\n",
    "    study = optuna.create_study(direction='maximize', pruner=pruner)\n",
    "\n",
    "    # Optimize with parallelization (adjust n_jobs based on available CPUs)\n",
    "    study.optimize(objective, n_trials=n_trials, n_jobs=8)\n",
    "\n",
    "    logging.info(\"Optuna Optimization Completed.\")\n",
    "    logging.info(f\"Best Trial: {study.best_trial.number}\")\n",
    "    logging.info(f\"Best Value: {study.best_trial.value}\")\n",
    "    logging.info(f\"Best Params: {study.best_trial.params}\")\n",
    "\n",
    "    # Save the best hyperparameters with 'value' column\n",
    "    os.makedirs('best_hyperparams', exist_ok=True)\n",
    "    best_params_df = pd.DataFrame([study.best_trial.params])\n",
    "    best_params_df.to_csv('best_hyperparams/best_params_ppo.csv', index=False)\n",
    "\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Updated backtest_pipeline Function\n",
    "# ===========================\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def backtest_pipeline(events_file, use_optuna=True, n_optuna_trials=20, trained_model=None):\n",
    "    \"\"\"\n",
    "    Executes the entire backtesting pipeline with optional Optuna hyperparameter tuning.\n",
    "\n",
    "    Parameters:\n",
    "    - events_file: String, path to the events CSV file.\n",
    "    - use_optuna: Bool, whether to perform hyperparameter tuning with Optuna.\n",
    "    - n_optuna_trials: Int, number of Optuna trials.\n",
    "    - penalty_factor: Float, penalty factor for impermanent loss.\n",
    "    - trained_model: Trained PPO model (if already trained).\n",
    "\n",
    "    Returns:\n",
    "    - metrics_df: DataFrame containing performance metrics for each strategy.\n",
    "    - dpd: DataFrame containing processed data.\n",
    "    \"\"\"\n",
    "    # Configure logging\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    # ===========================\n",
    "    # 1. Data Loading and Merging\n",
    "    # ===========================\n",
    "\n",
    "    # Load swap events data\n",
    "    swap_df = load_events(events_file)\n",
    "    logging.info(\"Swap events data loaded.\")\n",
    "\n",
    "    # Load macroeconomic data\n",
    "    macro_df = load_macroeconomic_data()\n",
    "    logging.info(\"Macroeconomic data loaded.\")\n",
    "\n",
    "    # Convert all macroeconomic columns to numeric, coercing errors\n",
    "    macro_df = macro_df.apply(pd.to_numeric, errors='coerce')\n",
    "    macro_df.fillna(method='ffill', inplace=True)\n",
    "    macro_df.fillna(method='bfill', inplace=True)\n",
    "    logging.info(\"Macroeconomic data converted to numeric.\")\n",
    "\n",
    "    # Merge swap events with macroeconomic data\n",
    "    swap_df.index = pd.to_datetime(swap_df.index.date)  # Ensure date format\n",
    "    merged_df = swap_df.join(macro_df, how='left')\n",
    "    merged_df.fillna(method='ffill', inplace=True)\n",
    "    merged_df.fillna(method='bfill', inplace=True)  # In case the first few rows have NaNs\n",
    "    logging.info(\"Swap events data merged with macroeconomic data.\")\n",
    "\n",
    "    # Convert relevant macroeconomic columns to float\n",
    "    macro_columns = [col for col in macro_df.columns if 'zscore' in col or 'fed_rate' in col or 'treasury_yield' in col or 'sp500_index' in col or 'stablecoin_mcap' in col]\n",
    "    for col in macro_columns:\n",
    "        if col in merged_df.columns:\n",
    "            merged_df[col] = pd.to_numeric(merged_df[col], errors='coerce')\n",
    "            merged_df[col].fillna(method='ffill', inplace=True)\n",
    "            merged_df[col].fillna(method='bfill', inplace=True)\n",
    "            logging.info(f\"Processed macroeconomic column: {col}\")\n",
    "        else:\n",
    "            logging.warning(f\"'{col}' column not found in merged_df.\")\n",
    "\n",
    "    # Remove unnecessary columns if any\n",
    "    columns_to_drop = ['blockNumber', 'transactionHash', 'sender', 'recipient', 'owner']\n",
    "    merged_df.drop(columns=[col for col in columns_to_drop if col in merged_df.columns], inplace=True, errors='ignore')\n",
    "    logging.info(\"Unnecessary columns removed.\")\n",
    "\n",
    "    # ===========================\n",
    "    # 2. Event Integration\n",
    "    # ===========================\n",
    "\n",
    "    # Integrate event data\n",
    "    merged_df = integrate_event_data(merged_df)\n",
    "    logging.info(\"Event data integrated.\")\n",
    "\n",
    "    # ===========================\n",
    "    # 3. Volatility Calculation\n",
    "    # ===========================\n",
    "\n",
    "    # Calculate volatility\n",
    "    merged_df = calculate_volatility(merged_df)\n",
    "    logging.info(\"Volatility calculated.\")\n",
    "\n",
    "    # ===========================\n",
    "    # 4. Create dpd DataFrame\n",
    "    # ===========================\n",
    "\n",
    "    # Create dpd DataFrame by aggregating events\n",
    "    dpd = create_dpd(merged_df)\n",
    "    logging.info(\"dpd DataFrame created.\")\n",
    "\n",
    "    # ===========================\n",
    "    # 5. Data Integrity Checks Before Training\n",
    "    # ===========================\n",
    "\n",
    "    # Check for NaNs and Infs in dpd\n",
    "    if dpd.isnull().values.any():\n",
    "        logging.warning(\"NaN values detected in dpd before training. Proceeding to fill.\")\n",
    "        dpd.fillna(0.0, inplace=True)\n",
    "\n",
    "    if np.isinf(dpd.values).any():\n",
    "        logging.warning(\"Inf values detected in dpd before training. Proceeding to replace.\")\n",
    "        dpd.replace([np.inf, -np.inf], 0.0, inplace=True)\n",
    "\n",
    "    # Additional Integrity Checks\n",
    "    # Ensure 'close' and 'ETH_price' are positive\n",
    "    if (dpd['close'] <= 0).any():\n",
    "        num_invalid = (dpd['close'] <= 0).sum()\n",
    "        logging.error(f\"'close' column contains {num_invalid} non-positive values. Correcting them.\")\n",
    "        dpd['close'] = dpd['close'].apply(lambda x: x if x > 0 else 1.0)  # Example correction\n",
    "\n",
    "    if ('ETH_price' in dpd.columns) and (dpd['ETH_price'] <= 0).any():\n",
    "        num_invalid = (dpd['ETH_price'] <= 0).sum()\n",
    "        logging.error(f\"'ETH_price' column contains {num_invalid} non-positive values. Correcting them.\")\n",
    "        dpd['ETH_price'] = dpd['ETH_price'].apply(lambda x: x if x > 0 else 1.0)  # Example correction\n",
    "\n",
    "    print(\"Data integrity check complete.\")\n",
    "    print(dpd.info())\n",
    "    print(dpd.head())\n",
    "\n",
    "    # ===========================\n",
    "    # 6. Build and Train Transformer Model\n",
    "    # ===========================\n",
    "\n",
    "    # Build and train Transformer model\n",
    "    transformer_model, dpd = build_and_train_transformer(\n",
    "        dpd,\n",
    "        transformer_model_path='transformer_model.keras'\n",
    "    )\n",
    "    logging.info(\"Transformer model trained and saved.\")\n",
    "\n",
    "    # ===========================\n",
    "    # 7. Hyperparameter Tuning with Optuna (Optional)\n",
    "    # ===========================\n",
    "\n",
    "    if use_optuna or trained_model is None:\n",
    "        logging.info(\"Starting hyperparameter tuning with Optuna.\")\n",
    "        study = run_optuna_optimization_ppo(dpd, n_trials=n_optuna_trials)\n",
    "        best_params = study.best_trial.params\n",
    "        logging.info(f\"Best Hyperparameters from Optuna: {best_params}\")\n",
    "    elif trained_model is not None:\n",
    "        logging.info(\"Using the provided trained PPO model for backtesting.\")\n",
    "        best_params = None  # Parameters are not needed when using a trained model\n",
    "    else:\n",
    "        # Use default or previously saved hyperparameters\n",
    "        best_params_path = 'best_hyperparams/best_params_ppo.csv'\n",
    "        if os.path.isfile(best_params_path):\n",
    "            best_params_df = pd.read_csv(best_params_path)\n",
    "            best_params = best_params_df.iloc[0].to_dict()\n",
    "            logging.info(f\"Loaded Best Hyperparameters: {best_params}\")\n",
    "        else:\n",
    "            logging.info(\"Using default hyperparameters for PPO.\")\n",
    "            best_params = {\n",
    "                'learning_rate': 1e-4,\n",
    "                'gamma': 0.99,\n",
    "                'gae_lambda': 0.95,\n",
    "                'ent_coef': 0.01,\n",
    "                'clip_range': 0.2,\n",
    "                'n_steps': 2048,\n",
    "                'batch_size': 64,\n",
    "                'n_epochs': 10,\n",
    "                'vf_coef': 0.5,\n",
    "                'max_grad_norm': 0.5\n",
    "            }\n",
    "\n",
    "    # ===========================\n",
    "    # 8. Train the PPO Agent with Best Hyperparameters\n",
    "    # ===========================\n",
    "\n",
    "    drl_model_path = 'ppo_lp_rebalance.zip'\n",
    "    if not os.path.isfile(drl_model_path) and trained_model is None:\n",
    "        env_rl = DummyVecEnv([lambda: LPRebalanceEnv(\n",
    "            dpd=dpd,\n",
    "            initial_capital=10.0,\n",
    "            transaction_cost=0.003,\n",
    "            allocation_fraction=0.9,\n",
    "        )])\n",
    "\n",
    "        # Initialize PPO Model with Best Hyperparameters\n",
    "        model_ppo = PPO(\n",
    "            'MlpPolicy',\n",
    "            env_rl,\n",
    "            learning_rate=float(best_params.get('learning_rate', 1e-4)),\n",
    "            gamma=float(best_params.get('gamma', 0.99)),\n",
    "            gae_lambda=float(best_params.get('gae_lambda', 0.95)),\n",
    "            ent_coef=float(best_params.get('ent_coef', 0.01)),\n",
    "            clip_range=float(best_params.get('clip_range', 0.2)),\n",
    "            n_steps=int(best_params.get('n_steps', 2048)),\n",
    "            batch_size=int(best_params.get('batch_size', 64)),\n",
    "            n_epochs=int(best_params.get('n_epochs', 10)),\n",
    "            vf_coef=float(best_params.get('vf_coef', 0.5)),\n",
    "            max_grad_norm=float(best_params.get('max_grad_norm', 0.5)),\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # Setup Callbacks\n",
    "        checkpoint_callback = CheckpointCallback(save_freq=10000, save_path='./models/ppo/', name_prefix='ppo_checkpoint')\n",
    "        eval_callback = EvalCallback(env_rl, best_model_save_path='./models/ppo/best_model/',\n",
    "                                     log_path='./logs/ppo/', eval_freq=5000,\n",
    "                                     deterministic=True, render=False)\n",
    "\n",
    "        # Train the Model\n",
    "        model_ppo.learn(total_timesteps=2000000, callback=[checkpoint_callback, eval_callback])\n",
    "\n",
    "        # Save the Final Model\n",
    "        model_ppo.save(drl_model_path)\n",
    "\n",
    "        # Ensure the environment's scaler is saved\n",
    "        env_rl.envs[0].save_scaler('scaler.pkl')\n",
    "\n",
    "        logging.info(\"PPO model trained and saved.\")\n",
    "    elif trained_model is None:\n",
    "        logging.info(\"PPO model already exists. Loading the trained model.\")\n",
    "        model_ppo = PPO.load(drl_model_path)\n",
    "    else:\n",
    "        model_ppo = trained_model\n",
    "\n",
    "    # ===========================\n",
    "    # 9. Implement Strategies\n",
    "    # ===========================\n",
    "\n",
    "    # Implement strategies using the trained PPO model\n",
    "    metrics_df, dpd = implement_strategies(\n",
    "        dpd=dpd,\n",
    "        initial_capital=10.0,\n",
    "        model=model_ppo,  # Pass the trained PPO model to implement_strategies\n",
    "    )\n",
    "    logging.info(\"Strategies implemented successfully.\")\n",
    "\n",
    "    return metrics_df, dpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configure root logger to write to a file and set level to WARNING\n",
    "logging.basicConfig(\n",
    "    level=logging.WARNING,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"training_logs.log\"),\n",
    "        # logging.StreamHandler()  # Optional: Remove or comment out to disable stdout logging\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Further control specific loggers if needed:\n",
    "logging.getLogger('stable_baselines3').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 14. Main Execution: Running the Backtest with Singular Scaler\n",
    "# ===========================\n",
    "\n",
    "# Define the path to your swap events data\n",
    "swap_events_file = os.path.join('data', 'wsteth_eth_events.csv')\n",
    "\n",
    "# Run the backtesting pipeline\n",
    "metrics_df, dpd = backtest_pipeline(\n",
    "    swap_events_file,\n",
    "    use_optuna=True,\n",
    "    n_optuna_trials=4,\n",
    "    trained_model='None'\n",
    ")\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
